{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG application from scratch\n",
    "\n",
    "Here is a high-level overview of the system we want to build:\n",
    "\n",
    "<img src='images/system1.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the environment variables we need to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# This is the YouTube video we're going to use.\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=lXUZvyajciY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "Let's define the LLM model that we'll use as part of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the model by asking a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The Los Angeles Dodgers won the World Series during the COVID-19 pandemic. They defeated the Tampa Bay Rays in six games in the 2020 World Series.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 21, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-80063e32-e9f5-4eee-b6f5-a6f9802a6f56-0', usage_metadata={'input_tokens': 21, 'output_tokens': 32, 'total_tokens': 53})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The Covid-19 pandemic, commonly referred to as the Covid Pandemic, was a global outbreak of the novel coronavirus SARS-CoV-2 that began in late 2019. The virus causes the respiratory illness known as Covid-19. The pandemic led to widespread illness, death, and economic disruption around the world, resulting in numerous restrictions, lockdowns, and public health measures to control the spread of the virus. The World Health Organization declared the Covid-19 outbreak a global pandemic on March 11, 2020.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 107, 'prompt_tokens': 14, 'total_tokens': 121, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5f191f0a-1d8e-4ef9-a152-8acf1d145499-0', usage_metadata={'input_tokens': 14, 'output_tokens': 107, 'total_tokens': 121})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What was the Covid Pandemic?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from the model is an `AIMessage` instance containing the answer. We can extract this answer by chaining the model with an [output parser](https://python.langchain.com/docs/how_to/#output-parsers).\n",
    "\n",
    "Here is what chaining the model with an output parser looks like:\n",
    "\n",
    "<img src='images/chain1.png' width=\"1200\">\n",
    "\n",
    "For this example, we'll use a simple `StrOutputParser` to extract the answer as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Los Angeles Dodgers won the World Series during the COVID-19 pandemic in 2020. They defeated the Tampa Bay Rays in six games to win their first championship since 1988.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing prompt templates\n",
    "\n",
    "We want to provide the model with some context and the question. [Prompt templates](https://python.langchain.com/docs/how_to/#prompt-templates) are a simple way to define and reuse prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Mary\\'s sister is Susana\\n\\nQuestion: Who is Mary\\'s sister?\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='\\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: {context}\\n\\nQuestion: {question}\\n'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now chain the prompt with the model and the output parser.\n",
    "\n",
    "<img src='images/chain2.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Susana'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Mary's sister is Susana\",\n",
    "    \"question\": \"Who is Mary's sister?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining chains\n",
    "\n",
    "We can combine different chains to create more complex workflows. For example, let's create a second chain that translates the answer from the first chain into a different language.\n",
    "\n",
    "Let's start by creating a new prompt template for the translation chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate {answer} to {language}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new translation chain that combines the result from the first chain with the translation prompt.\n",
    "\n",
    "Here is what the new workflow looks like:\n",
    "\n",
    "<img src='images/chain3.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'María tiene una hermana.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "translation_chain = (\n",
    "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
    ")\n",
    "\n",
    "translation_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Mary's sister is Susana. She doesn't have any more siblings.\",\n",
    "        \"question\": \"How many sisters does Mary have?\",\n",
    "        \"language\": \"Spanish\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Marie a une sœur.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Mary's sister is Susana. She doesn't have any more siblings.\",\n",
    "        \"question\": \"How many sisters does Mary have?\",\n",
    "        \"language\": \"French\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing the YouTube Video\n",
    "\n",
    "The context we want to send the model comes from a YouTube video. Let's download the video and transcribe it using [OpenAI's Whisper](https://openai.com/research/whisper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] No supported JavaScript runtime could be found. Only deno is enabled by default; to use another runtime add  --js-runtimes RUNTIME[:PATH]  to your command/config. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one\n",
      "WARNING: [youtube] lXUZvyajciY: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
      "WARNING: [youtube] lXUZvyajciY: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "import os, tempfile\n",
    "import yt_dlp, whisper\n",
    "\n",
    "MODEL = \"medium\"\n",
    "\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        ydl = yt_dlp.YoutubeDL({\n",
    "            \"format\": \"bestaudio/best\",\n",
    "            \"outtmpl\": f\"{tmpdir}/%(id)s.%(ext)s\",\n",
    "            \"quiet\": True,\n",
    "        })\n",
    "        info = ydl.extract_info(YOUTUBE_VIDEO, download=True)\n",
    "        file = next(os.path.join(tmpdir, f) for f in os.listdir(tmpdir)\n",
    "                    if f.endswith((\".m4a\", \".webm\", \".mp3\", \".opus\")))\n",
    "\n",
    "        whisper_model = whisper.load_model(MODEL)\n",
    "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "        with open(\"transcription.txt\", \"w\") as out:\n",
    "            out.write(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the transcription and display the first few characters to ensure everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reinfersional learning is terrible. It just so happens that everything that we had before is much wo'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the entire transcription as context\n",
    "\n",
    "If we try to invoke the chain using the transcription as context, the model will return an error because the context is too long.\n",
    "\n",
    "Large Language Models support limitted context sizes. The video we are using is too long for the model to handle, so we need to find a different solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 37181 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    chain.invoke({\n",
    "        \"context\": transcription,\n",
    "        \"question\": \"Can you explain what Heated Rivalry book is about?\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the transcription\n",
    "\n",
    "Since we can't use the entire transcription as the context for the model, a potential solution is to split the transcription into smaller chunks. We can then invoke the model using only the relevant chunks to answer a particular question:\n",
    "\n",
    "<img src='images/system2.png' width=\"1200\">\n",
    "\n",
    "Let's start by loading the transcription in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content=\"Reinfersional learning is terrible. It just so happens that everything that we had before is much worse. I'm actually optimistic. I think this will work. I think it's tractable. I'm only sounding pessimistic because when I go on my Twitter timeline, I see all this stuff. That makes no sense to me. A lot of it is, I think, honestly just fundraising. We're not actually building animals. We're building ghosts. These sort of ethereal spirit entities, because they're fully digital, and they're kind of like mimicking humans. And it's a different kind of intelligence. It's business as usual, because we're in an intelligence explosion already and have been for decades. Everything is gradually being automated, has been for hundreds of years. Don't write blog posts. Don't do slides. Don't do any of that. Build the code. Arrange it. Get it to work. It's the only way to go. Otherwise, you're missing knowledge. If you have a perfect AI tutor, maybe you can get extremely far. The geniuses of today are bare discussion to surface of what a human mind can do, I think. Today, I'm speaking with Andre Karpathy. Andre, why do you say that this will be the decade of agents and not the year of agents? Well, first of all, thank you for having me here. Excited to be here. So the quote that you've just mentioned, it's the decade of agents, that's actually a reaction to an existing pre-existing quote, I should say, where I think some of the labs, I'm not actually sure who said this, but they were alluding to this being the year of agents with respect to LLMs and how they were going to evolve. And I think I was triggered by that, because I feel like there's some overpredictions going on in the industry. And in my mind, this is really a lot more accurately described as the decade of agents. And we have some very early agents that are actually extremely impressive that I use daily, Claude and Codex and so on. But I still feel like there's so much work to be done. And so I think my reaction is like, we'll be working with these things for a decade. They're going to get better, and it's going to be wonderful. But I think I was just reacting to the timelines, I suppose, of the implication. And what do you think will take a decade to accomplish? What are the bottlenecks? Well, actually make it work. So in my mind, when you're talking about an agent, I guess, or what the labs have in mind and what maybe I have in mind as well, is you should think of it almost like an employee or like an intern that you would hire to work with you. So for example, you work with some employees here. When would you prefer to have an agent, like Claude or Codex, do that work? Currently, of course, they can't. What would it take for them to be able to do that? Why don't you do it today? And the reason you don't do it today is because they just don't work. So they don't have enough intelligence, they're not multimodal enough, they can't do computer use and all this kind of stuff. And they don't do a lot of the things that you've alluded to earlier. They don't have continual learning. You can't just tell them something and they'll remember it. And they're just cognitively lacking and it's just not working. And I just think that it will take about a decade to work through all of those issues. Interesting. So as a professional podcaster and a viewer of AI From Afar, it's easy to identify for me, like, oh, here's what's lacking. Continual learning is lacking or multimodality is lacking. But I don't really have a good way of trying to put a timeline on it. Like if somebody's like, how long will continual learning take? There's no prior I have about this is a project that should take five years, 10 years, 50 years. Why a decade? Why not one year? Why not 50 years? Yeah, I guess this is where you get into a bit of my own intuition a little bit and also just kind of doing a bit of an extrapolation with respect to my own experience in the field. So I guess I've been in AI for almost two decades. I mean, it's going to be maybe 15 years or so, not that long. You had Richard Sutton here, who was all around, of course, for much longer. But I do have about 15 years of experience of people making predictions, of seeing how they actually turned out. And also, I was in the industry for a while, and I was in research. And I worked in the industry for a while. So I guess I kind of have just a general intuition that I have left from that. And I feel like the problems are tractable. They're surmountable. But they're still difficult. And if I just average it out, it just kind of feels like a ticket, I guess, to me. This is actually quite interesting. I want to hear not only the history, but what people in the room felt was about to happen at various different breakthrough moments. What were the ways in which their feelings were either overly pessimistic or overly optimistic? Should we just go through each of them one by one? Yeah, I mean, that's a giant question, because of course, you're talking about 15 years of stuff that happened. I mean, AI is actually so wonderful, because there have been a number of, I would say, seismic shifts that where the entire field has suddenly looked a different way. And I guess I've maybe lived through two or three of those. And I still think there will continue to be some, because they come with some kind of almost surprising regularity. Well, when my career began, of course, when I started to work on deep learning, when I became interested in deep learning, this was just kind of like by chance of being right next to Jeff Hinton at the University of Toronto. And Jeff Hinton, of course, is kind of like the godfather figure of AI. And he was training all these neural networks, and I thought it was incredible and interesting. But this was not like the main thing that everyone in AI was doing by far. This was a niche little subject on the side. That's kind of maybe like the first dramatic sort of seismic shift that came with the AlexNet and so on. I would say AlexNet reoriented everyone, and everyone started to train neural networks. But it was still very per task, per specific task. So maybe I have an image classifier, or I have a neural machine translator, or something like that. And people became very slowly actually interested in basically agents, I would say. And people started to think, OK, well, maybe we have a check mark next to the visual cortex, or something like that. But what about the other parts of the brain? And how can we get an actual full agent or full entity that can actually interact in the world? And I would say the Atari sort of deep reinforcement learning shift in 2013 or so was part of that early effort of agents in my mind. Because it was an attempt to try to get agents that not just perceive the world, but also take actions and interact and get rewards from environments. And at the time, this was Atari games. And I kind of feel like that was a misstep, actually. And it was a misstep that actually even the early open AI that I was part of, of course, kind of adopted. Because at that time, the zeitgeist was reinforcement learning environments, games, game playing, beat games, get lots of different types of games. And open AI was doing a lot of that. So that was maybe like another prominent part of, I would say, AI where maybe for two or three or four years, everyone was doing reinforcement learning on games. And basically, that was a little bit of a misstep. And what I was trying to do at open AI, actually, is I was always a little bit suspicious of games as being like this thing that would actually lead to AGI. Because in my mind, you want something like an accountant or something that's actually interacting with the real world. And I just didn't see how games kind of add up to it. And so my project at open AI, for example, was within the scope of the universe project on an agent that was using keyboard and mouse to operate web pages. And I really wanted to have something that interacts with the actual digital world that can do knowledge work. And it just so turns out that this was extremely early, way too early, so early that we shouldn't have been working on that. Because if you're just stumbling your way around and keyboard mashing and mouse clicking and trying to get rewards in these environments, your reward is too sparse. And you just won't learn. And you're going to burn a forest computing. And you're never actually going to get something off the ground. And so what you're missing is this power of representation in the neural network. And so for example, today, people are training those computer using agents. But they're doing it on top of a large language model. And so you actually have to get the language model first. You have to get the representations first. And you have to do that by all the pre-training and all the LLM stuff. So I kind of feel like maybe, loosely speaking, it was like people keep maybe trying to get the full thing too early a few times, where people really try to go after agents too early, I would say. And that was Atari and Universe, and even my own experience. And you actually have to do some things first before you sort of get to those agents. And maybe now the agents are a lot more competent. But maybe we're still missing some parts of that stack. But I would say maybe those are like the three major buckets of what people were doing, training neural nets for tasks, trying to the first round of agents, and then maybe the LLMs, and actually seeking the representation power of the neural networks before you tack on everything else on top. Interesting. Yeah, I guess if I were to steel man the sort of sudden perspective would be that humans actually can just take on everything at once. Even animals can take on everything at once. Animals are maybe a better example, because they don't even have the scaffold of language. They just get thrown out into the world. And they just have to make sense of everything without any labels. And the vision for AGI then should just be something which just looks at sensory data, looks at the computer screen, and it just figures out what's going on from scratch. I mean, if a human was put in a similar situation, that would be trained from scratch. But I mean, this is like a human growing up, or an animal growing up. So why shouldn't that be the vision for AI rather than this thing where we're doing millions of years of training? I think that's a really good question. I mean, so Sutton was on your podcast, and I saw the podcast. And I had a write up about that podcast almost that gets into a little bit of how I see things. And I kind of feel like I'm very careful to make analogies to animals, because they came about by very different optimization process. Animals are evolved, and they actually come with a huge amount of hardware that's built in. And when, for example, my example in the post was the zebra, a zebra gets born, and a few minutes later, it's running around and following its mother. That's an extremely complicated thing to do. That's not reinforcement learning. That's something that's baked in. So this is a very interesting question, obviously, as some way of encoding the weights of our neural nets in ATCGs. And I have no idea how that works, but it apparently works. So I kind of feel like brains just came from a very different process. And I am very hesitant to take inspiration from it, because we're not actually running that process. So in my post, I kind of said, we're not actually building animals. We're building ghosts, or spirits, or whatever people want to call it, We're not doing training by evolution. We're doing training by basically imitation of humans and the data that they've put on the internet. And so you end up with these sort of ethereal spirit entities, because they're fully digital, and they're mimicking humans. And it's a different kind of intelligence. If you imagine a space of intelligences, we're starting off at a different point almost. We're not really building animals. But I think it's also possible to make them a bit more animal-like over time. And I think we should be doing that. And so I kind of feel like, sorry, just one more point is, I do feel like Sutton basically has a very, his framework is we want to build animals. And I actually think that would be wonderful. If we can get that to work, that would be amazing. If there was a single algorithm that you can just run on the internet, and it learns everything, that would be incredible. I almost suspect that I'm not actually sure that it exists. And that's certainly actually not what animals do, because animals have this outer loop of evolution. And a lot of what looks like learning is actually a lot more maturation of the brain. And I think there's actually very little reinforcement learning for animals. And I think a lot of the reinforcement learning is actually more like motor tasks. It's not intelligence tasks. So I actually kind of think humans don't actually really use RL, roughly speaking, is what I would say. Can you repeat the last sentence? A lot of that intelligence is not motor tasks. That's why, sorry. A lot of the reinforcement learning in my perspective would be things that are a lot more like motor-like, like simple kind of like tasks, throwing a hoop, or something like that. But I don't think that humans use reinforcement learning for a lot of intelligence tasks, like problem solving and so on. Interesting. That doesn't mean we shouldn't do that for research. But I just feel like that's what animals do or don't. I'm going to take a second to digest that, because there's a lot of different ideas. Maybe one clarifying question I can ask to understand the perspective. So I think you suggest that, look, evolution is doing the kind of thing that pre-training does in the sense of building something which can then understand the world. The difference, I guess, is that evolution has to be titrated, in the case of humans, through three gigabytes of DNA. And so that's very unlike the weights of a model. I mean, literally, the weights of the model are a brain, which obviously is not encoded in the sperm and the egg, or does not exist in the sperm and the egg. So it has to be grown. And also, the information for every single synapse in the brain simply cannot exist in the three gigabytes that exist in the DNA. Evolution seems closer to finding the algorithm, which then does the lifetime learning. Now, maybe the lifetime learning is not analogous to RL, to your point. Is that compatible with the thing you were saying, or would you disagree with that? I think so. I would agree with you that there's some miraculous compression going on, because obviously the weights of the neural net are not stored in the TCGs. There's some kind of a dramatic compression, and there's some kind of learning algorithms encoded that take over and do some of the learning online. So I definitely agree with you on that. Basically, I would say I'm a lot more practically minded. I don't come at it from the perspective of let's build animals. I come from the perspective of let's build useful things. So I have a hard hat on. And I'm just observing that, look, we're not going to do evolution, because I don't know how to do that. But it does turn out we can build these ghost spirit-like entities by imitating internet documents. This works. And it's actually kind of like it's a way to bring you up to something that has a lot of sort of built-in knowledge and intelligence in some way, similar to maybe what evolution has done. So that's why I kind of call pre-training this kind of crappy evolution. It's like the practically possible version with our technology and what we have available to us to get to a starting point where we can actually do things like reinforcement learning and so on. Just to steelman the other perspective, because after doing this in an interview and thinking about it a bit, here's an important point here. Evolution does not give us the knowledge, really. It gives us the algorithm to find the knowledge. And that seems different from pre-training. So perhaps the perspective is that pre-training helps build the kind of entity which can learn better. It teaches meta-learning. And therefore, it is similar to finding an algorithm. But if it's like evolution gives us knowledge, pre-training gives us knowledge, that analogy seems to break down. So it's subtle. And I think you're right to push back on it. But basically, the thing that pre-training is doing, so you're basically getting the next token predictor over the internet. And you're training that into a neural net. It's doing two things, actually, that are kind of unrelated. Number one, it's picking up all this knowledge, as I call it. Number two, it's actually becoming intelligent. By observing the algorithmic patterns in the internet, it actually boots up all these little circuits and algorithms inside the neural net to do things like in-context learning and all this kind of stuff. And actually, you don't actually need or want the knowledge. I actually think that's probably actually holding back the neural networks overall, because it's actually getting them to rely on the knowledge a little too much sometimes. For example, I kind of feel like agents, one thing they're not very good at is going off the data manifold of what exists on the internet. If they had less knowledge or less memory, actually, maybe they would be better. And so what I think we have to do going forward, this would be part of the research paradigms, is actually think we need to figure out ways to remove some of the knowledge and to keep what I call this cognitive core. Is this intelligent entity that is kind of stripped from knowledge, but contains the algorithms and contains the magic of intelligence and problem solving and the strategies of it and all this kind of stuff. There's so much interesting stuff there. So let's start with in-context learning. This is an obvious point, but I think it's worth just saying it explicitly and meditating on it. The situation in which these models seem the most intelligent, in which they are like, I talk to them and I'm like, wow, there's really something on the other end that's responding to me thinking about things. If it makes a mistake, it's like, oh, wait, that's actually the wrong way to think about it. I'm backing up. All that is happening in context. That's where I feel like the real intelligence you can visibly see. And that in-context learning process is developed by gradient descent on pre-training. It spontaneously meta learns in-context learning. But the in-context learning itself is not gradient descent in the same way that our lifetime intelligence as humans to be able to do things is conditioned by evolution. But our actual learning during our lifetime is happening through some other process. I actually don't fully agree with that, but you should continue with that. OK. Actually, then I'm very curious to understand how that analogy breaks down. I think I'm hesitant to say that in-context learning is not doing gradient descent because it's not doing explicit gradient descent. But I still think that in-context learning, basically, it's pattern completion within a token window. And it just turns out that there's a huge amount of patterns on the internet. And so you're right. The model learns to complete the pattern. And that's inside the weights. The weights of the neural network are trying to discover patterns and complete the pattern. And there's some kind of an adaptation that happens inside the neural network, which is kind of magical and just falls out from internet just because there's a lot of patterns. I will say that there have been some papers that I thought were interesting that actually look at the mechanisms behind in-context learning. And I do think it's possible that in-context learning actually runs a small gradient descent loop internally in the layers of the neural network. And so I recall one paper in particular where they were doing linear regression, actually, using in-context learning. So basically, your inputs into the neural network are xy pairs, xy, xy, xy, that happen to be on the line. And then you do x, and you expect the y. And the neural network, when you train it in this way, actually does do linear regression. And normally, when you would run linear regression, you have a small gradient descent optimizer that basically looks at xy, looks at an error, calculates the gradient of the weights, and does the update a few times. It just turns out that when they looked at the weights of that in-context learning algorithm, they actually found some analogies to gradient descent mechanics. In fact, I think even the paper was stronger because they actually hard-coded the weights of the neural network to do gradient descent through attention and all the internals of the neural network. So I guess that's just my only pushback is that who knows how in-context learning works. But I actually think that it's probably doing a little bit of some kind of funky gradient descent internally. And I think that that's possible. So I guess I was only pushing back on, you're saying it's not doing in-context learning. Who knows what it's doing. But it's probably maybe doing something similar to it. But we don't know. So then it's worth thinking about, OK, if both of them are implementing gradient descent. If in-context learning and pre-training are both implementing something like gradient descent, why does it feel like in-context learning, actually, we're getting to this continual learning, real intelligence like thing, whereas you don't get the analogous feeling just from pre-training? At least, you could argue that. And so if it's the same algorithm, what could be different? Well, one way you could think about it is how much information does the model store per information it receives from training? And if you look at pre-training, if you look at Lama 3, for example, I think it's trained on 15 trillion tokens. And if you look at the 70B model, that would be the equivalent of 0.07 bits per token in that it sees in pre-training in terms of the information in the weights of the model compared to the tokens it reads. Whereas if you look at the KV cache and how it grows per additional token in in-context learning, it's like 320 kilobytes. So that's a 35 million-fold difference in how much information per token is assimilated by the model. I wonder if that's relevant at all. I think I kind of agree. I mean, the way I usually put this is that anything that happens during the training of the neural network, the knowledge is only kind of like a hazy recollection of what happened in the training time. And that's because the compression is dramatic. You're taking 15 trillion tokens and you're compressing it to just your final network of a few billion parameters. So obviously, it's a massive amount of compression going on. So I kind of refer to it as like a hazy recollection of the internet documents. Whereas anything that happens in the context window of the neural network, you're plugging all the tokens and it's building up all this KV cache representation, is very directly accessible to the neural net. So I compare the KV cache and the stuff that happens at test time to more like a working memory. Like all the stuff that's in the context window is very directly accessible to the neural net. So there's always these almost surprising analogies between LLMs and humans. And I find them kind of surprising because we're not trying to build a human brain, of course, just directly. We're just finding that this works and we're doing it. But I do think that anything that's in the weights, it's kind of like a hazy recollection of what you read a year ago. Anything that you give it as a context at test time is directly in the working memory. And I think that's a very powerful analogy to things for things. So when you, for example, go to an LLM and you ask it about some book and what happened in it, like Nicolay's book or something like that, the LLM will often give you some stuff, which is roughly correct. But if you give it the full chapter and ask it questions, you're going to get much better results because it's now loaded in the working memory of the model. So I basically agree with your very long way of saying that I kind of agree and that's why. Stepping back, what is it the part about human intelligence that we have the most feel to replicate with these models? I almost feel like just a lot of it. So maybe one way to think about it, I don't know if this is the best way, but I almost kind of feel like, again, making these analogies imperfect as they are. We've stumbled by, with the transformer neural network, which is extremely powerful, very general. You can train transformers on audio or video or text or whatever you want. And it just learns patterns. And they're very powerful and it works really well. That to me almost indicates that this is kind of like some piece of cortical tissue, something like that, because the cortex is famously very plastic as well. You can rewire parts of brains. And there was the slightly gruesome experiments with rewiring visual cortex to the auditory cortex and this animal like learn fine, et cetera. So I think that this is kind of like cortical tissue. I think when we're doing reasoning and planning inside the neural networks, so basically doing reasoning traces for thinking models, that's kind of like the prefrontal cortex. And then I think maybe those are like little check marks. But I still think there's many brain parts and nuclei that are not explored. So maybe, for example, there's a basic idea doing a bit of reinforcement learning when we fine tune the models on reinforcement learning. But whereas like the hippocampus, not obvious what that would be. Some parts are probably not important. Maybe the cerebellum is not important to cognition, it's thought, so maybe we can skip some of it. But I still think that's, for example, the amygdala, all the emotions and instincts. And there's probably like a bunch of other nuclei in the brain that are very ancient that I don't think we've really replicated. I don't actually know that we should be pursuing the building of an analog of a human brain. Again, an engineer mostly at heart. But I still feel like maybe another way to answer the question is you're not going to hire this thing as an intern. And it's missing a lot of it's because it comes with a lot of these cognitive deficits that we all intuitively feel when we talk to the models. And so it's just like not fully there yet. You can look at it as like not all the brain parts are checked off yet. This is maybe relevant to the question of thinking about how fast these issues will be solved. So sometimes people will say about continual learning, look, actually you could easily replicate this capability. Just as in-context learning emerged spontaneously as a result of pre-training, continual learning over longer horizons will emerge spontaneously if the model is incentivized to recollect information over longer horizons or horizons longer than one session. So if there's some outer loop RL, which has many sessions within that outer loop, then this continual learning where it uses like it fine-tunes itself or it writes to an external memory or something will just sort of like emerge spontaneously. Do you think things are any other plausible? I don't have really a prior over how plausible is that? How likely is that to happen? I don't know that I fully resonate with that because I feel like these models, when you boot them up and they have zero tokens in the window, they're always like restarting from scratch where they were. So I don't actually know in that worldview what it looks like because again, maybe making some analogies to humans just because I think it's roughly concrete and kind of interesting to think through. I feel like when I'm awake, I'm building up a context window of stuff that's happening during the day. But I feel like when I go to sleep, something magical happens where I don't actually think that that context window stays around. I think there's some process of distillation into weights of my brain. And this happens during sleep and all this kind of stuff. We don't have an equivalent for all that in large language models. And that's to me more adjacent to when you talk about continual learning and so on as absent. These models don't really have this distillation phase of taking what happened, analyzing it, obsessively thinking through it, basically doing some kind of a synthetic data generation process and distilling it back into the weights and maybe having a specific neural net per person. Maybe it's a LoRa, it's not a full weight neural network. It's just some of the small sparse subset of the weights are changed. But basically we do want to create ways of creating these individuals that have very long contexts. It's not only remaining in the context window because the context windows grow very, very long. Like maybe we have some very elaborate sparse attention over it. But I still think that humans obviously have some process for distilling some of that knowledge into the weights. We're missing it. And I do also think that humans have some kind of a very elaborate sparse attention scheme, which I think we're starting to see some early hints of. So DeepSeq v3.2 just came out and I saw that they have like a sparse attention as an example. And this is one way to have very, very long context windows. So I almost feel like we are redoing a lot of the cognitive tricks that evolution came up with through a very different process. But we're, I think, gonna converge on a similar architecture cognitively. Interesting. In 10 years, do you think it'll still be something like a transformer but with a much more modified attention and more sparse MLPs and so forth? Well, the way I like to think about it is, OK, let's translation invariance in time, right? So 10 years ago, where were we? 2015, we had convolutional neural networks primarily. Residual networks just came out. So remarkably similar, I guess, but quite a bit different still. Transformer was not around. All these sort of like more modern tweaks on the transformer were not around. So maybe some of the things that we can bet on, I think, in 10 years by translational sort of equivalence is we're still training giant neural networks with forward, backward, passing update through gradient descent. But maybe it looks a little bit different. And it's just everything is much bigger. Actually, recently, I also went back all the way to 1989, which was kind of a fun exercise for me a few years ago, because I was reproducing Jan Lacoon's 1989 convolutional network, which was the first neural network I'm aware of trained via gradient descent, like modern neural network trained gradient descent on digit recognition. And I was just interested in, OK, how can I modernize this? How much of this is algorithms? How much of this is data? How much of this progress is compute and systems? And I was able to very quickly half the learning rate, just knowing by time travel by 33 years. So if I time travel by algorithms to 33 years, I could adjust what Jan Lacoon did in 1989, and I could basically half the error. But to get further gains, I had to add a lot more data. I had to 10x the training set. And then I had to actually add more computational optimizations. I had to basically train for much longer with dropout and other regularization techniques. And so it's almost like all these things have to improve simultaneously. So we're probably going to have a lot more data. We're probably going to have a lot better hardware. Probably going to have a lot better kernels and software. We're probably going to have better algorithms. And all of those, it's almost like no one of them is winning too much. All of them are surprisingly equal. And this has kind of been the trend for a while. So I guess to answer maybe your question, I expect differences algorithmically to what's happening today. But I do also expect that some of the things that have stuck around for a very long time will probably still be there. It's probably still a giant neural network trained with gradient descent. That would be my guess. It's surprising that all of those things together are only halved half the error, which is like 30 years of progress. Maybe half is a lot, because if you halve the error, that actually means that. Half is a lot. Yeah. But I guess what was shocking to me is everything needs to improve across the board. Architecture optimized a loss function and also has improved across the board forever. So I kind of expect all those changes to be alive and well. Yeah, actually, I was about to ask a very similar question about NanoChat. Because since you just coded up recently, every single sort of step in the process of building a chatbot is like fresh in your RAM. And I'm curious if you had similar thoughts about, oh, there was no one thing that was relevant to going from GPT-2 to NanoChat. What are sort of surprising takeaways from the experience? On building NanoChat? So NanoChat is a kind of repository I released, was it yesterday or the day before? I can't remember. We can see this lead generation that went into the. Well, it's trying to be the simplest complete repository that covers the whole pipeline end to end of building a chat GPT clone. And so you have all of the steps, not just any individual step, which is a bunch of. I worked on all the individual steps sort of in the past and released small pieces of code that could show you how that's done in algorithmic sense in simple code. But this kind of handles all the entire pipeline. I think in terms of learning, it's not so much, I don't know, that I actually found something that I learned from it necessarily. I kind of already had in my mind as how you build it. And this is just the process of mechanically building it and making it clean enough so that people can actually learn from it and that they find it useful. Yeah. What is the best way for somebody to learn from it? Is it just like delete all the code and try to implement from scratch, try to add modifications to it? Yeah, I think that's a great question. I would probably say, so basically it's about 1,000 lines of code that takes you through the entire pipeline. I would probably put it on the right monitor. Like if you have two monitors, you put it on the right. And you want to build it from scratch. You build it from start. You're not allowed to copy paste. You're allowed to reference. You're not allowed to copy paste. Maybe that's how I would do it. But I also think the repository by itself, it is like a pretty large beast. I mean, when you write this code, you don't go from top to bottom. You go from chunks and you grow the chunks. And that information is absent. Like you wouldn't know where to start. And so I think it's not just the final repository that's needed, it's like the building of the repository, which is a complicated chunk growing process. So that part is not there yet. I would love to actually add that probably later this week or something in some way. Like either it's probably a video or something like that. But maybe, roughly speaking, that's what I would try to do. Build the stuff yourself, but don't allow yourself copy paste. I do think that there's two types of knowledge almost. There's the high level surface knowledge. But the thing is that when you actually build something from scratch, you're forced to come to terms with what you don't actually understand and you don't know that you don't understand it. And it always leads to a deeper understanding. And it's just the only way to build. It's like, if I can't build it, I don't understand it. Is that Feynman code, I believe, or something along those lines? 100%, I've always believed this very strongly. Because there's all these micro things that are just not properly arranged and you don't really have the knowledge. You just think you have the knowledge. So don't write blog posts. Don't do slides. Don't do any of that. Like build the code, arrange it, get it to work. It's the only way to go. Otherwise, you're missing knowledge. You tweeted out that coding models were actually of very little help to you in assembling this repository. And I'm curious why that was. Yeah. So the repository, I guess I built it over a period of a bit more than a month. And I would say there's like three major classes of how people interact with code right now. Some people completely reject all of LLMs. And they are just writing by scratch. I think this is probably not the right thing to do anymore. The intermediate part, which is where I am, is you still write a lot of things from scratch. But you use the autocomplete. That's basically available now from these models. So when you start writing out a little piece of it, it will all complete for you. And you can just tap through. And most of the time, it's correct. Sometimes it's not. And you edit it. But you're still very much the architect of what you're writing. And then there's the vibe coding. Hi, please implement this or that. Enter. And then let the model do it. And that's the agents. I do feel like the agents work in very specific settings. And I would use them in specific settings. Again, these are all tools available to you. And you have to learn what they're good at, and what they're not good at, and when to use them. So the agents are actually pretty good, for example, if you're doing boilerplate stuff. Boilerplate code that's just copy-paste stuff, they're very good at that. They're very good at stuff that occurs very often on the internet, because there's lots of examples of it in the training sets of these models. So there's features of things where the models will do very well. I would say NanoChat is not an example of this, because it's a fairly unique repository. There's not that much code, I think, in the way that I've structured it. And it's not boilerplate code. It's actually intellectually intense code, almost. And everything has to be very precisely arranged. And the models were always trying to, they kept trying to, I mean, they have so many cognitive deficits, right? So one example, they keep trying to, they keep misunderstanding the code, because they have too much memory from all the typical ways of doing things on the internet that I just wasn't adopting. So the models, for example, I mean, I don't know if I want to get into the full details, but they keep thinking I'm writing normal code, and I'm not. Maybe one example. Maybe one example is, so the way to synchronize, so we have eight GPUs that are all doing forward-backwards. The way to synchronize gradients between them is to use a distributed data parallel container of PyTorch, which automatically does all the, as you're doing the backward, it will start communicating and synchronizing gradients. I didn't use DDP, because I didn't want to use it, because it's not necessary. So I threw it out. And I basically wrote my own synchronization routine that's inside the step of the optimizer. And so the models were trying to get me to use the DDP container. And they were very concerned about, OK, this gets way too technical. But I wasn't using that container, because I don't need it, and I have a custom implementation of something like it. And they just couldn't internalize it. You had your own implementation. Yeah, they couldn't get past that. And then they kept trying to mess up the style. They're way too over-defensive. They make all these try-catch statements. They keep trying to make a production code base. And I have a bunch of assumptions in my code, and it's OK. And it's just like, I don't need all this extra stuff in there. And so I just kind of feel like they're bloating the code base. They're bloating the complexity. They keep misunderstanding. They're using deprecated APIs a bunch of times. So it's total mess. And it's just not that useful. I can go in, and I can clean it up, but it's not that useful. I also feel like it's kind of annoying to have to type out what I want in English, because it's just too much typing. If I just navigate to the part of the code that I want, and I go where I know the code has to appear, and I start typing out the first three letters, autocomplete gets it and just gives you the code. And so I think this is a very high-information bandwidth to specify what you want. If you point to the code where you want it, and you type out the first few pieces, and the model will complete it. So I guess what I mean is I think these models are good in certain parts of the stack. I actually use the models a little bit in. There are two examples where I actually use the models that I think are illustrative. One was when I generated the report, that's actually more boilerplate-y. So I actually bytecoded partially some of that stuff. That was fine, because it's not like mission critical stuff, and it works fine. And then the other part is when I was rewriting the tokenizer in Rust, I'm actually not as good at Rust, because I'm fairly new to Rust. So there's a bit of bytecoding going on when I was writing some of the Rust code. But I had Python implementation that I fully understand, and I'm just making sure I'm making a more efficient version of it, and I have tests. So I feel safer doing that stuff. And so basically, they lower or increase accessibility to languages or paradigms that you might not be as familiar with. So I think they're very helpful there as well, because there's a ton of Rust code out there. The models are actually pretty good at it. I happen to not know that much about it, so the models are very useful there. The reason I think this question is so interesting is because the main story people have about AI exploding and getting to superintelligence pretty rapidly is AI automating, AI engineering, and AI research. And so they'll look at the fact that you can have cloud code and make entire application, crud applications from scratch and be like, if you had this capability inside of OpenAI and DeepMind and everything, well, just imagine the level of just 1,000 of you or a million of you in parallel finding little architectural tweaks. And so it's quite interesting to hear you say that this is the thing they're sort of asymmetrically worse at. And it's quite relevant to forecasting whether the AI 2027 type explosion is likely to happen any time soon. I think that's a good way of putting it. I think you're getting at some of my, like, why my timelines are a bit longer. You're right. I think, yeah, they're not very good at code that hasn't never been written before. Maybe there's one way to put it, which is what we're trying to achieve when we're building these models. Very native question, but the architectural tweaks that you're adding to NanoChat, they're in a paper somewhere, right? They might even be in a repo somewhere. So is it surprising that they aren't able to integrate that into whenever you're, like, add rope embeddings or something? They do that in the wrong way? It's tough. I think they kind of know, but they don't fully know. And they don't know how to fully integrate it into the repo and your style and your code and your place and some of the custom things that you're doing and how it fits with all the assumptions of the repository and all this kind of stuff. So I think they do have some knowledge, but they haven't gotten to the place where they can actually integrate it, make sense of it, and so on. I do think that a lot of this stuff, by the way, continues to improve. So I think currently, probably state-of-the-art model that I go to is the GPT-5 Pro. And that's a very, very powerful model. So if I actually have 20 minutes, I will copy paste my entire repo and I go to GPT-5 Pro, the Oracle, for some questions. And often, it's not too bad and surprisingly good compared to what existed a year ago. But I do think that overall, the models are not there. And I kind of feel like the industry, it's making too big of a jump. And it's trying to pretend like this is amazing, and it's not. It's slop. And I think they're not coming to terms with it, and maybe they're trying to fundraise or something like that. I'm not sure what's going on. But we're at this intermediate stage. The models are amazing. They still need a lot of work. For now, autocomplete is my sweet spot. But sometimes, for some types of code, I will go to a null-imagent. Yeah. Actually, here's another reason why this is really interesting. Through the history of programming, there's been many productivity improvements, compilers, linting, better programming languages, et cetera, which have increased programmer productivity but have not led to an explosion. So that sounds very much like autocomplete tab. And this other category is just automation of the programmer. And it's interesting you're seeing more in the category of the historical analogies of better compilers or something. Maybe because this gets at one other kind of thought of that I do feel like I have a hard time differentiating where AI begins and stops. Because I do see AI as fundamentally an extension of computing in some pretty fundamental way. And I feel like I see a continuum of this kind of recursive self-improvement or of speeding up programmers all the way from the beginning. Even I would say code editors, syntax highlighting, syntax or checking even of the types, like data type checking. All these kinds of tools that we've built for each other, even search engines. Like why aren't search engines part of AI? I don't know, ranking is kind of AI, right? At some point, Google was like, even early on, they were thinking of themselves as an AI company doing Google search engine, which I think is totally fair. And so I kind of see it as a lot more of a continuum than I think other people do. And it's hard for me to draw the line. And I kind of feel like, OK, we're now getting a much better autocomplete. And now we're also getting some agents which are kind of like these loopy things, but they kind of go off rails sometimes. And what's going on is that the human is progressively doing a bit less and less of the low level stuff. For example, we're not writing the assembly code because we have compilers. Compilers will take my high level language and see and write the assembly code. So we're abstracting ourselves very, very slowly. And there's this what I call autonomy slider of more and more stuff is automated, of the stuff that can be automated at any point in time. And we're doing a bit less and less and raising ourselves in the layer of abstraction over the automation. One of the big problems with RL is that it's incredibly information sparse. Labelbox can help you with this by increasing the amount of information that your agent gets to learn from with every single episode. For example, one of their customers wanted to train a coding agent. So Labelbox augmented an IDE with a bunch of extra data collection tools and staffed a team of expert software engineers from their aligner network to generate trajectories that were optimized for training. Now, obviously, these engineers evaluated these interactions on a pass fail basis. But they also rated every single response on a bunch of different dimensions, like readability and performance. And they wrote down their thought processes for every single rating that they gave. So you're basically showing every single step an engineer takes and every single thought that they have while they're doing their job. And this is just something you could never get from usage data alone. And so Labelbox packaged up all these evaluations and included all the agent trajectories and the corrective human edits for the customer to train on. This is just one example. So go check out how Labelbox can get you high quality frontier data across domains, modalities, and training paradigms. Reach out at labelbox.com slash thwarkash. Let's talk about RL a bit. You treated us some very interesting things about this. Conceptually, how should we think about the way that humans are able to build a rich world model just from interacting with our environment and in ways that seems almost irrespective of the final reward at the end of the episode. If somebody is starting to start a business, and at the end of 10 years she finds out whether the business succeeded or failed, we say that she's earned a bunch of wisdom and experience. But it's not because the log probes of every single thing that happened over the last 10 years are upweighted or downweighted. It's something much more deliberate and rich is happening. What is the ML analogy? And how does that compare to what we're doing with LLMs right now? Yeah, maybe the way I would put it is humans don't use reinforcement learning, is maybe what I've said. I think they do something different, which is, yeah, you experience. So reinforcement learning is a lot worse than I think the average person thinks. Reinforcement learning is terrible. It just so happens that everything that we had before is much worse. Because previously, we were just imitating people, so it has all these issues. So in reinforcement learning, say you're working with, you're solving a math problem. This is very simple. You're given a math problem, and you're trying to find the solution. Now, in reinforcement learning, you will try lots of things in parallel first. So you're given a problem. You try hundreds of different attempts. And these attempts can be complex, right? They can be like, oh, let me try this. Let me try that. This didn't work. That didn't work, et cetera. And then maybe you get an answer. And now you check the back of the book, and you see, OK, the correct answer is this. And then you can see that, OK, this one, this one, and that one got the correct answer, but these other 97 of them didn't. So literally what reinforcement learning does is it goes to the ones that worked really well. And every single thing you did along the way, every single token gets up-weighted of, like, do more of this. The problem with that is, I mean, people will say that your estimator has high variance, but I mean, it's just noisy. It's noisy. So basically, it kind of almost assumes that every single little piece of the solution that you made that righted the right answer was the correct thing to do, which is not true. Like, you may have gone down the wrong alleys until you righted the right solution. Every single one of those incorrect things you did, as long as you got to the correct solution, will be up-weighted as do more of this. It's terrible. It's noise. You've done all this work only to find a single, at the end, you get a single number of, like, oh, you did correct. And based on that, you weigh that entire trajectory as, like, up-weight or down-weight. And so the way I like to put it is you're sucking supervision through a straw, because you've done all this work that could be a minute's of rollout, and you're, like, sucking the bits of supervision of the final reward signal through a straw, and you're, like, putting it, you're, like, basically, like, yeah, you're broadcasting that across the entire trajectory and using that to up-weight or down-weight that trajectory. It's just crazy. A human would never do this. Number one, a human would never do hundreds of rollouts. Number two, when a person sort of finds a solution, they will have a pretty complicated process of review, of like, OK, I think these parts that I did well, these parts I did not do that well, I should probably do this or that. And they think through things. There's nothing in current LLMs that does this. There's no equivalent of it. But I do see papers popping out that are trying to do this, because it's obvious to everyone in the field. So I kind of see it as, like, the first imitation learning, actually, by the way, was extremely surprising and miraculous and amazing that we can fine-tune by imitation of humans. And that was incredible, because in the beginning, all we had was base models. Base models are autocomplete. And it wasn't obvious to me at the time, and I had to learn this, and the paper that, like, blew my mind was Instruct GPT, because it pointed out that, hey, you can take the pre-trained model, which is autocomplete. And if you just fine-tune it on text that looks like conversations, the model will very rapidly adapt to become very conversational. And it keeps all the knowledge from pre-training. And this blew my mind, because I didn't understand that it's just, like, stylistically can adjust so quickly and become an assistant to a user through just a few loops of fine-tuning on that kind of data. It was very miraculous to me that that worked. So incredible. And that was, like, two years, three years of work. And now came RL. And RL allows you to do a bit better than just imitation learning, right? Because you can't have these reward functions, and you can hill climb on the reward functions. And so some problems have just correct answers. You can hill climb on that without getting expert trajectories to imitate. So that's amazing. And the model can also discover solutions that a human might never come up with. So this is incredible. And yet, it's so stupid. So I think we need more. And so I saw a paper from Google yesterday that tried to have this reflect and review idea in mind. What was the memory bank paper or something? I don't know. I've actually seen a few papers along these lines. So I expect there to be some kind of a major update to how we do algorithms for LLMs coming in that realm. And then I think we need three or four or five more. Something like that. You're so good at coming up with evocative phrases. Sucking supervision through a straw is so good. Why hasn't, so you're saying, like, your problem with outcome-based reward is that you have this huge trajectory. And then at the end, you're trying to learn every single possible thing about what you should do and what you should learn about the world from that one final bit. Why hasn't, given the fact that this is obvious, why hasn't process-based supervision as an alternative been a successful way to make models more capable? What has been preventing us from using this alternative paradigm? So process-based supervision just refers to the fact that we're not going to have a reward function only at the very end of, after you've made 10 minutes of work, I'm not going to tell you you did well or not well. I'm going to tell you at every single step of the way how well you're doing. And this is basically the reason we don't have that is not tricky how you do that properly. Because you have partial solutions and you don't know how to assign credit. So when you get the right answer, it's just an equality match to the answer. Very simple to implement. If you're doing basically process supervision, how do you assign in an automatable way partial credit assignment? It's not obvious how you do it. Lots of labs, I think, are trying to do it with these LLM judges. So basically, you get LLMs to try to do it. So you prompt an LLM, hey, look at a partial solution of a student. How well do you think they're doing if the answer is this? And they try to tune the prompt. The reason that I think this is kind of tricky is quite subtle. And it's the fact that any time you use an LLM to assign a reward, those LLMs are giant things with billions of parameters, and they're gameable. And if you're reinforcement learning with respect to them, you will find adversarial examples for your LLM judges almost guaranteed. You can't do this for too long. You do maybe 10 steps or 20 steps, maybe it will work. But you can't do 100 or 1,000 because it's not obvious. Because I understand it's not obvious, but basically, the model will find little cracks. It will find all these spurious things in the nooks and crannies of the giant model and find a way to cheat it. So one example that's prominently in my mind is, I think this was probably public. But basically, if you're using an LLM judge for a reward, so you just give it a solution from a student and ask it if the student did well or not. We were training with reinforcement learning against that reward function. And it worked really well. And then suddenly, the reward became extremely large. It was a massive jump, and it did perfect. And you're looking at it like, wow, this means the student is perfect in all these problems. It's fully solved math. But actually, what's happening is that when you look at the completions that you're getting from the model, they are complete nonsense. They start out OK, and then they change to the, the, the, the, the, the, the. So it's just like, oh, OK, let's take 2 plus 3, and we do this and this, and then the, the, the, the, the, the, the, the. And you're looking at it, and it's like, this is crazy. How is it getting a reward of 1 or 100%? And you look at the LLM judge, and it turns out that the, the, the, the, the, is an adversarial example for the model. And it assigns 100% probability to it. And it's just because this is an out-of-sample example to the LLM. It's never seen it during training. And you're in pure generalization land. It's never seen it during training. And in the pure generalization land, you can find these examples that break it. You're basically training the LLM to be a prompt injection model. Not even that. Prompt injection is way too fancy. You're finding adversarial examples, as they're called. These are nonsensical solutions that are obviously wrong. But the model, things are amazing. So today, saying you think this is the bottleneck to making RL more functional, then that will require making LLMs better judges, if you want to do this in an automated way. And then so is it just going to be like some sort of GAN-like approach, or you had to train models to be more robust? Yeah. I think the labs are probably doing all that. OK, so the obvious thing is the, the, the, should not get 100% reward. OK, well, take the, the, the, the. Put in the training set of the LLM judge and say, this is not 100%, this is 0%. You can do this. But every time you do this, you get a new LLM, and it still has adversarial examples. There's infinity adversarial examples. And I think probably, if you iterate this a few times, it'll probably be harder and harder to find adversarial examples. But I'm not 100% sure, because this thing has a trillion parameters or whatnot. So I bet you the, the labs are trying. I don't actually, I still think, I still think we need other ideas. Interesting. Do you have some shape of what the other idea could be? So like this, this idea of like a review, review solution encompass synthetic examples, such that when you train on them, you get, you get better. And like, meta-learn it in some way. And I think there's some papers that I'm starting to see pop out. I only am at a stage of like reading abstracts, because a lot of these papers, you know, they're just ideas. Someone has to actually like make it work on a frontier LLM lab scale in full generality. Because when you see these papers, they pop up, and it's just like a little bit of noisy. You know, it's cool ideas. But I haven't actually seen anyone convincingly show that this is possible. That said, the LLM labs are fairly closed. So who knows what they're doing now. But yeah. So I guess I see a very, not easy, but like I can conceptualize how you would be able to train on synthetic examples or synthetic problems that you have made for yourself. But there seems to be another thing humans do. Maybe sleep is this. Maybe daydreaming is this. Which is not necessarily come up with fake problems, but just like reflect. And I'm not sure what the ML analogy for daydreaming or sleeping, but just reflecting. I haven't come up with any problem. I mean, obviously the very basic analogy should be like fine tuning on reflection bits. But I feel like in practice, that probably wouldn't work that well. So I don't know if you have some take on what the analogy of like this thing is. Yeah. I do think that we're missing some aspects there. So as an example, when you're reading a book, I almost feel like currently when LLMs are reading a book, what that means is we stretch out the sequence of text. And the model is predicting the next token. And it's getting some knowledge from that. That's not really what humans do. So when you're reading a book, I almost don't even feel like the book is like exposition I'm supposed to be attending to and training on. The book is a set of prompts for me to do synthetic data generation. Or for you to get into a book club and talk about it with your friends. And it's by manipulating that information that you actually gain that knowledge. And I think we have no equivalent of that, again, with LLMs. They don't really do that. But I'd love to see during pre-training some kind of a stage that thinks through the material and tries to reconcile it with what it already knows and thinks through for some amount of time and gets that to work. And so there's no equivalence of any of this. This is all research. There's some very subtle that I think are very hard to understand reasons why it's not trivial. So if I can just describe one, why can we just synthetically generate and train on it? Well, because every synthetic example, like if I just give synthetic generation of the model thinking about a book, you look at it and you're like, this looks great. Why can't I train on it? Well, you could try, but the model will actually get much worse if you continue trying. And that's because all of the samples you get from models are silently collapsed. They're silently, it is not obvious if you look at any individual example of it, they occupy a very tiny manifold of the possible space of thoughts about content. So the LLMs, when they come off, they're what we call collapsed. They have a collapsed data distribution. If you sample, one easy way to see it is go to ChatGPT and ask it, tell me a joke. It only has three jokes. It's not giving you the whole breadth of possible jokes. It's giving you like, it knows like three jokes. They're silently collapsed. So basically, you're not getting the richness and diversity and the entropy from these models as you would get from humans. So humans are a lot more sort of noisier, but at least they're not biased. They're not in a statistical sense. They're not silently collapsed. They maintain a huge amount of entropy. So how do you get synthetic data generation to work despite the collapse and while maintaining the entropy is a research problem? Just to make sure I understood, the reason that the collapse is relevant to synthetic data generation is because you want to be able to come up with synthetic problems or reflections which are not already in your data distribution? I guess what I'm saying is, say we have a chapter of a book and I ask a null to think about it. It will give you something that looks very reasonable. But if I ask it 10 times, you'll notice that all of them are the same. You can't just keep scaling, quote, unquote, reflection on the same amount of prompt information and then get returns from that. So any individual sample will look OK, but the distribution of it is quite terrible. And it's quite terrible in such a way that if you continue training on too much of your own stuff, you actually collapse. I actually think that there's no fundamental solutions to this possibly. And I also think humans collapse over time. I think this is, again, these analogies are surprisingly good. But humans collapse during the course of their lives. This is why children have completely, they haven't overfit yet. And they will say stuff that will shock you, because you can see where they're coming from, but it's just not the thing people say, because they're not yet collapsed. But we're collapsed. We end up revisiting the same thoughts. We end up saying more and more of the same stuff. And the learning rates go down. And the collapse continues to get worse. And then everything deteriorates. Have you seen this super interesting paper that dreaming is a way of preventing this kind of overfitting and collapse? The reason dreaming is evolutionary adaptive is to put you in weird situations that are very unlike your day-to-day reality, to prevent this kind of overfitting? That's an interesting idea. I do think that when you're generating things in your head and then you're attending to it, you're training on your own samples. You're training on your synthetic data. And if you do it for too long, you go off rails. And you collapse way too much. So you always have to seek entropy in your life. So talking to other people is a great source of entropy. Things like that. So maybe the brain has also built some internal mechanisms for increasing the amount of entropy in that process. But yeah, maybe that's an interesting idea. This is a very ill-formed thought. So I'll just put it out and let you react to it. The best learners that we are aware of, which are children, are extremely bad at recollecting information. In fact, in the very earliest stages of childhood, you will forget everything. You're just an amnesiac about everything that happens before a certain year date. But you're extremely good at picking up new languages and learning from the world. And maybe there's some element of being able to see the forest for the trees. Whereas if you compare it to the opposite end of the spectrum, you have LLM pre-training, which these models will literally be able to regurgitate word for word what is the next thing in a Wikipedia page. But their ability to learn abstract concepts really quickly the way a child can is much more limited. And then adults are somewhere in between where they don't have the flexibility of childhood learning. But adults can memorize facts and information in a way that is harder for kids. And I don't know if there's something interesting about that spectrum. I think there's something very interesting about that. Yeah, 100%. I do think that humans actually, they do have a lot more of an element compared to LLMs of seeing the forest for the trees. And we're not actually that good at memorization, which is actually a feature. Because we're not that good at memorization, we actually are forced to find the patterns in a more general sense. I think LLMs in comparison are extremely good at memorization. They will recite passages from all these training sources. You can give them completely nonsensical data. You can hash some amount of text or something like that. You get a completely random sequence. If you train on it, even just I think a single iteration or two, it can suddenly regurgitate the entire thing. It will memorize it. There's no way a person can read a single sequence of random numbers and recite it to you. And that's a feature, not a bug almost, because it forces you to only learn the generalizable components. Whereas LLMs are distracted by all the memory that they have of the pre-training documents. And it's probably very distracting to them in a certain sense. So that's why when I talk about the cognitive core, I actually want to remove the memory, which is what we talked about. I'd love to have them have less memory so that they have to look things up. And they only maintain the algorithms for thought and the idea of an experiment and all this cognitive glue of acting. And this is also relevant to preventing model collapse. Let me think. I'm not sure. I think it's almost like a separate axis. It's almost like the models are way too good at memorization. And somehow we should remove that. And I think people are much worse, but it's a good thing. What is a solution to model collapse? So there's very naive things you could attempt. It's just like the distribution over logics should be wider or something. There's many naive things you could try. What ends up being the problem with the naive approaches? Yeah, I think that's a great question. You can imagine having a regularization for entropy and things like that. I guess they just don't work as well empirically. Because right now, the models are collapsed. But I will say most of the tasks that we want of them don't actually demand the diversity. That's probably the answer of what's going on. And so it's just that the frontier labs are trying to make the models useful. And I kind of just feel like the diversity of the outputs number one, it's much harder to work with and evaluate and all this kind of stuff. But maybe it's not what's actually capturing most of the value. In fact, it's actively penalized. If you're super creative in RL, it's not good. Or maybe if you're doing a lot of writing help from LLMs and stuff like that, I think it's probably bad. Because the models will give you these silently all the same stuff. So they won't explore lots of different ways of answering a question. But I kind of feel like maybe this diversity is just not as big of a, maybe not as many applications needed so the models don't have it. But then it's actually a problem with synthetic generation time, et cetera. So we're actually shooting ourselves in the foot by not allowing this entropy to maintain in the model. And I think possibly the labs should try harder. And then I think you hinted that it's a very fundamental problem. It won't be easy to solve. And what's your intuition for that? I don't actually know if it's super fundamental. I don't actually know if I intended to say that. I do think that I haven't done these experiments. But I do think that you could probably regularize the entropy to be higher. So you're encouraging the model to give you more and more solutions. But you don't want it to start deviating too much from the training data. It's going to start making up its own language. It's going to start using words that are extremely rare. So it's going to drift too much from the distribution. So I think controlling the distribution is just like a tricky, it's just like someone just has to, it's probably not trivial in that sense. How many bits should the optimal core of intelligence end up being if you just had to make a guess? The thing we put on the von Neumann probes, how big does it have to be? So it's really interesting in the history of the field, because at one point, everything was very scaling-pilled in terms of like, oh, we're going to make much bigger models, trillions of parameter models. And actually what the models have done in size is they've gone up. And now they've actually kind of like actually even come down. Their models are smaller. And even then, I actually think they memorized way too much. So I think I had a prediction a while back that I almost feel like we can get cognitive cores that are very good at even like a billion parameters. It should be already like, if you talk to a billion parameter model, I think in 20 years, you can actually have a very productive conversation, it thinks. And it's a lot more like a human. But if you ask it some factual question, it might have to look it up. But it knows that it doesn't know, and it might have to look it up, and it will just do all the reasonable things. That's actually surprising that you think it will take a billion. Because already we have a billion parameter models, or a couple billion parameter models that are very intelligent. Well, some of the models are like a trillion parameters, right? But they remember so much stuff. Yeah. But I'm surprised that in 10 years, given the pace, OK, we have GPT, OSS, 20b. That's way better than GPT-4 original, which was a trillion plus parameters. So given that trend, I'm actually surprised you think in 10 years, the cognitive core is still a billion parameters. Yeah, I'm surprised you're not like, it's going to be like tens of millions or millions. No, because I basically think that the training data is, so here's the issue. The training data is the internet, which is really terrible. So there's a huge amount of gains to be made because the internet is terrible. Like if you actually, and even the internet, when you and I think of the internet, you're thinking of like a Wall Street Journal, or that's not what this is. When you're actually looking at a pre-training data set in the front of your lab, and you look at a random internet document, it's total garbage. Like I don't even know how this works at all. It's some like stock ticker symbols. It's a huge amount of slop and garbage from all the corners of the internet. It's not like your Wall Street Journal article that's extremely rare. So I almost feel like because the internet is so terrible, we actually have to sort of build really big models to compress all that. Most of that compression is memory work instead of cognitive work. But what we really want is the cognitive part, actually delete the memory. And then, so what I'm saying is like, we need intelligent models to help us refine even the pre-training set to just narrow it down to the cognitive components. And then I think get away with a much smaller model, because it's a much better data set, and you could train it on it. But probably it's not trained directly on it. It's probably distilled from a much better model still. But why is a distilled version still a billion? Is I guess the thing I'm curious about. I just feel like distillation works extremely well. So almost every small model, if you have a small model, it's almost certainly distilled. Why would you train on? Right, no, no, but why is the distillation in 10 years not getting below one billion? Oh, you think it should be smaller than a billion? I mean, come on, right? I don't know. At some point, it should take at least a billion knobs to do something interesting. You're thinking it should be even smaller? Yeah, I mean, just like if you look at the trend over the last few years, just finding low-hanging fruit and going from like trillion plus models that are like literally two orders of magnitude smaller in a matter of two years and having better performance. It makes me think that the core of intelligence might be even way, way smaller. Like plenty of room at the bottom to paraphrase Feynman. I mean, I almost feel like I'm already contrarian by talking about a billion parameter cognitive core and you're outdoing me. I think, yeah, maybe we could get a little bit smaller. I mean, I still think that there should be enough. Yeah, maybe it can be smaller. I do think that practically speaking, you want the model to have some knowledge. You don't want it to be looking up everything. Because then you can't think in your head, you're looking up way too much stuff all the time. So I do think it needs to be some basic curriculum needs to be there for knowledge. But it doesn't have esoteric knowledge. So we're discussing what plausibly could be the cognitive core. There's a separate question, which is what will actually be the size of French models over time? And I'm curious to have predictions. So we had increasing scale up to maybe 4.5. And now we're seeing decreasing slash plateauing scale. There's many reasons that could be going on. But do you have a prediction about going forward? Will the biggest models be bigger? Will they be smaller? Will they be the same? Yeah, I don't know that I have a super strong prediction. The labs are just being practical. They have a FLOPs budget and a cost budget. And it just turns out that pre-training is not where you want to put most of your FLOPs or your cost. So that's why the models have gotten smaller. Because they are a bit smaller. The pre-training stage is smaller, et cetera. But they make it up in reinforcement learning and all this kind of stuff, mid-training and all this kind of stuff that follows. So they're just being practical in terms of all the stages and how you get the most bang for the buck. So I guess forecasting that trend, I think, is quite hard. I do still expect that there's so much longing for it. That's my basic expectation. And so I have a very wide distribution here. Do you expect there looking for it to be similar in kind to the kinds of things that have been happening over the last two to five years? Like, just in terms of, like, if I look at NanoChat versus NanoGPT and then the architectural tweaks you made, is that basically the flavor of things you continue to keep happening? You're not expecting any giant fair time shift? I expect the data sets to get much, much better. Because when you look at the average data sets, they're extremely terrible. Like, so bad that I don't even know how anything works, to be honest. Look at the average example in the training set. Like, factual mistakes, errors, nonsensical things. Somehow when you do it at scale, the noise washes away and you're left with some of the signal. So data sets will improve a ton. It's just everything gets better. So our hardware, our older kernels, older kernels for running the hardware and maximizing what you get with the hardware. So NVIDIA is slowly tuning the actual hardware itself, TensorCore and so on. All that needs to happen and will continue to happen. All the kernels will get better and utilize the chip to the max extent. All the algorithms will probably improve over optimization architecture and just all of the modeling components of how everything is done and what the algorithms are that we're even training with. So I do kind of expect, like, just very everything. Nothing dominates everything plus 20%. Right. This is roughly what I've seen. OK, this is my general manager, Max. Good to be here, here every day. And you have been here since you were onboarded about six months ago. But when I was- Eight months ago. Oh, right. Time passes so fast. But when I onboarded you, I was in France. And so we basically didn't get the chance to talk at all almost. And you basically just gave me one login. I gave you access to my Mercury platform, which is the banking platform that I was using at the time to run the podcast. And so I logged into Mercury assuming that that would just be the first of many steps. And then I realized that was how you were running the entire business, even down to a lot of our editors, our international contractors. And so you had just figured out how to set up these recurring payments to set up basic payroll. I mean, Mercury made the experience of all of these things I was doing before so seamless that it didn't even occur to me until you pointed it out that this is not the natural way to set up payroll or invoicing or any of these other things. Yeah, I was surprised. But I was like, it's worked so far. That's right, yeah. So maybe I'll trust it. And then now I can't think of doing anything else. All right, you heard him. Visit mercury.com to apply online in minutes. Cool, thanks, Max. Thanks for having me. Dude, you're great at this. I'm so nervous, but thank you. Mercury is a financial technology company, not a bank. Banking services provided through Choice Financial Group, Column NA, and Evolved Bank and Trust, members FDIC. People have proposed different ways of charting how much progress you've made towards full AGI. Because if you can come up with some line, then you can see where that line intersects with AGI and where that would happen on the x-axis. And so people have proposed, oh, it's like the education level. We had a high schooler, and then they went to college with RL, and they're going to get a PhD. Yeah, I don't like that one. Or then they'll propose Horizon Link. So maybe they can do tasks that take a minute. They can do those autonomously. Then they can autonomously do tasks that take a human an hour, a human a week, et cetera. How do you think about what is the relevant y-axis here? How should we think about how AI is making progress? So I guess I have two answers to that. Number one, I'm almost tempted to reject the question entirely, because again, I see this as an extension of computing. Have we talked about how to chart progress in computing? Or how do you chart progress in computing since 1970s or whatever? What is the x-axis? So I kind of feel like the whole question is kind of funny from that perspective a little bit. But I will say, I guess, when people talk about AI and the original AGI and how we spoke about it when opening I started, AGI was a system you can go to that can do any task that is economically valuable, any economically valuable task at human performance or better. OK, so that was the definition. And I was pretty happy with that at the time. And I kind of feel like I've stuck to that definition forever. And then people have made up all kinds of other definitions. But I feel like I like that definition. Now number one, the first concession that people make all the time is they just take out all the physical stuff, because we're just talking about digital knowledge work. I feel like that's a pretty major concession compared to the original definition, which was like any task a human can do. I can lift things, et cetera. I can't do that, obviously. So OK, but we'll take it. What fraction of the economy are we taking away by saying only knowledge work? I don't actually know the numbers. I feel like it's about 10% to 20% if I had to guess, is only knowledge work. Like someone could work from home and perform tasks, something like that. I still think it's a really large market. Like what is the size of the economy and what is 10%, 20%? Like we're still talking about a few trillion dollars even in the US of market share almost, or like work. So still a very massive bucket. But I guess like going back to the definition, I guess what I would be looking for is to what extent is that definition true? So are there jobs or lots of tasks? If we think of tasks as not jobs, but tasks, kind of difficult. Because the problem is like society will refactor based on the tasks that make up jobs compared to what's based on what's automatable or not. But today, what jobs are replaceable by AI? So a good example recently was Jeff Hinton's prediction that radiologists would not be a job anymore. And this turned out to be very wrong in a bunch of ways. So radiologists are alive and well and growing, even though computer vision is really, really good at recognizing all the different things that they have to recognize in images. And it's just messy, complicated job with a lot of surfaces and dealing with patients and all this kind of stuff in the context of it. So I guess I don't actually know that by that definition, AI has made a huge amount of dent yet. But some of the jobs maybe that I would be looking for have some features that I think make it very amenable to automation earlier than later. As an example, call center employees often come up and I think rightly so. Because call center employees have a number of simplifying properties with respect to what's automatable today. Their jobs are pretty simple. It's a sequence of tasks and every task looks similar. Like you take a phone call with a person, it's 10 minutes of interaction or whatever it is, probably a bit longer. In my experience, a lot longer. And you complete some task in some scheme and you change some database entries around or something like that. So you keep repeating something over and over again and that's your job. So basically you do wanna bring in the task horizon, how long it takes to perform a task. And then you want to also remove context. Like you're not dealing with different parts of services of companies or other customers. It's just the database, you and a person you're serving. And so it's more closed, it's more understandable and it's purely digital. So I would be looking for those things. But even there, I'm not actually looking at full automation yet. I'm looking for an autonomy slider. And I almost expect that we are not gonna instantly replace people. We're going to be swapping in AIs that do 80% of the volume. They delegate 20% of the volume to humans. And humans are supervising teams of five AIs doing the call center work. That's more rote. So I would be looking for new interfaces or new companies that provide some kind of a layer that allows you to manage some of these AIs. That are not yet perfect. And then I would expect that across the economy. And a lot of jobs are a lot harder than call center employee. I wonder with radiologists, I'm totally speculating. I have no idea what the actual workflow of a radiologist involves. But one analogy that might be applicable is when we was first being ruled out, there'd be a person sitting in the front seat and you just had to have them there to make sure that if something went really wrong, they're there to monitor. And I think even today, people are still watching to make sure things are going well. RoboTaxi, which was just deployed, actually still has a person inside it. And we could be in a similar situation where if you automate 99% of a job, that last 1% that human has to do is incredibly valuable because it's bottlenecking everything else. And if it was the case with radiologists where the person sitting in the front of the Uber or the front of the Waymo has to be specially trained for years in order to be able to provide the last 1%, their wages should go up tremendously because they're the one thing bottlenecking wide deployment. So radiologists, I think their wages have gone up for similar reasons. If you're like the last bottleneck, you're like, and you're not fungible, which like a Waymo driver might be fungible with other things. So you might see this thing where like your wages go like, whoop, and then to get in that 8% and then like, just like that. And then the last 1% is gone. I see. And I wonder if we're seeing similar things with radiology or salaries of call center workers or anything like that. Yeah, I think that's an interesting question. I don't think we're currently seeing that with radiology and I don't have like, in my understanding, but I think radiology is not a good example basically. I don't know why Jeff Hinton picked on radiology because I think it's an extremely messy, complicated profession. So I would be a lot more interested in what's happening with call center employees today, for example, because I would expect a lot of the road stuff to be automatable today. And I don't have a first level access to it, but maybe I would be looking for trends of what's happening with the call center employees. Maybe some of the things I would also expect is maybe they are swapping in AI, but then I would still wait for a year or two because I would potentially expect them to pull back and actually rehire some of the people. I think there's been evidence that that's already been happening generally in companies that have been adopting AI, which I think is quite surprising. I also find what was really surprising, AGI, right? A thing which would do everything, okay, we'll take out physical work. So things which should be able to do all knowledge work. And what you would have naively anticipated that the way this progression would happen is like, you would take a little task that a consultant is doing, you take that out of the bucket. You take a little task that an accountant is doing, you take that out of the bucket. And then you're just doing this across all knowledge work. But instead, if we do believe we're on the path of AGI with the current paradigm, the progression is very much not like that. At least it just does not seem like consultants and accounts and whatever are getting like huge productive improvement. It's very much like programmers are like getting more and more chills with the way their work. If you're to look at the revenues of these companies, discounting just like normal chat revenue, which I think is like, I don't know, it that's similar to like Google or something. Just looking at API revenues, it's like dominated by coding, right? So this thing which is general, quote unquote, which should be able to do any knowledge work, it's just overwhelmingly doing only coding. And it's a surprising way that you would expect like the AGI to be deployed. So I think there's an interesting point here because I do believe coding is like the perfect first thing for these LLMs and agents. And that's because coding has always fundamentally worked around text. It's computer terminals and text and everything is based around text. And LLMs, the way they're trained on the internet, love text. And so they're perfect text processors and there's all this data out there and it's just perfect fit. And also we have a lot of infrastructure pre-built for handling code and text. So for example, we have a Visual Studio code or your favorite IDE showing you code. And an agent can plug into that. So for example, if an agent has a diff where it made some change, we suddenly have all this code already that shows all the differences to a code base using a diff. So it's almost like we've pre-built a lot of the infrastructure for code. Now contrast that with some of the things that don't enjoy that at all. So as an example, there's people trying to build automation, not for coding, but for example, for slides. Like I saw a company doing slides. That's much, much harder. And the reason it's much, much harder is because slides are not text. Slides are little graphics and they're arranged spatially and there's visual components to it. And slides don't have this pre-built infrastructure. Like for example, if an agent is to make a different change to your slides, how does a thing show you the diff? How do you see the diff? There's nothing that shows diffs for slides. Someone has to build it. So it's just some of these things are not amenable to AIs as they are, which is text processors. And code surprisingly is. I actually am not sure if that alone explains it because I personally have tried to get LLMs to be useful in domains which are just pure language and language out. Like rewriting transcripts, like coming up with clips based on transcripts, et cetera. And you might say, well, it's very plausible that I didn't do every single possible thing I could do. I put a bunch of good examples in context, but maybe I should have done like some kind of fine tuning, whatever. So our mutual friend, Andy Matuschak, told me that he actually tried 50 billion things to try to get models to be good at writing spaced repetition prompts. Again, very much language in, language out tasks. The kind of thing that should be dead center in the repertoire of these LLMs. And he tried in-context learning obviously with a few short examples. He tried, I think he told me like a bunch of things like supervised fine tuning and retrieval, whatever. And he just could not get them to make cards to a satisfaction. So I find it striking that even in language out domains, it's actually very hard to get a lot of economic value out of these models separate from coding. And I don't know what explains it. Yeah, I think that makes sense. I mean, I would say, yeah, I'm not saying that anything text is trivial. I do think that code is like, it's pretty structured. Text is maybe a lot more flowery in this. And there's a lot more like entropy in text, I would say. I don't know how else to put it. And also, I mean, code is hard. And so people sort of feel quite empowered by LLMs even from like simple kind of knowledge. I basically, I don't actually know that I have a very good answer. I mean, obviously like text makes it much, much easier maybe is maybe why I put it, but it doesn't mean that all text is trivial. How do you think about super intelligence? Do you expect it to feel qualitatively different from normal humans or human companies? I guess I see it as like a progression of automation in society, right? And again, like extrapolating the trend of computing, I just feel like there'll be a gradual automation of a lot of things and super intelligence will be sort of like the extrapolation of that. So I do think we expect more and more autonomous entities over time that are doing a lot of the digital work and then eventually even the physical work probably some amount of time later. But basically I see it as just automation, roughly speaking. I guess automation includes the things humans can already do and super intelligence supplies things to humans. Well, but some of the things that people do is invent new things, which I would just put into the automation if that makes sense. Yeah, but I guess maybe less abstractly and more sort of like qualitatively. Do you expect something to feel like, okay, this, because this thing can either think so fast or has so many copies or the copies can merge back in themselves or is quote unquote much smarter. Any number of advantages an AI might have, it will qualitatively, the civilization in which these AI systems will just feel qualitatively different from human civilization. I mean, it is fundamentally automation, but I mean, it will be like extremely foreign. I do think it will look really strange because like you mentioned, we can run all of this on a computer cluster, et cetera, and much faster and all this thing. I mean, maybe some of the scenarios, for example, that I start to get like nervous about with respect to when the world looks like that, is this kind of like gradual loss of control and understanding of what's happening. And I think that's actually the most likely outcome probably is that there'll be a gradual loss of understanding of, and we'll gradually layer all this stuff everywhere and there'll be fewer and fewer people who understand it. And then there will be a sort of this like scenario of a gradual loss of control and understanding of what's happening. That to me seems most likely outcome of how all of this stuff will go down. Let me provide that a bit. It's not clear to me that loss of control and loss of understanding are the same things. A board of directors at like whatever, TSMC, Intel, name around company, they're just like prestigious 80 year olds. They have very little understanding and maybe they don't practically actually have control. But, or actually maybe a better example is the president of the United States. President has a lot of fucking power. I'm not trying to make a good statement by the current operant, but maybe I am, but like the actual level of understanding is very different from the level of control. Yeah, I think that's fair. That's a good pushback. I think like, I guess I expect loss of both. How come? I mean, loss of understanding is obvious, but why loss of control? So we're really far into a territory of, I don't know what this looks like, but if I was to write sci-fi novels, they would look along the lines of not even a single like entity or something like that. That just sort of like takes over everything, but actually like multiple competing entities that gradually become more and more autonomous. And some of them go rogue and the others like fight them off and all this kind of stuff. And it's like this hot pot of completely autonomous activity that we've delegated to. I kind of feel like it would have that flavor. It is not the fact that they are smarter than us that is resulting in the loss of control. It is the fact that they are competing with each other and whatever arises out of that competition, at least with the loss of control. I mean, I basically expect there to be, I mean, a lot of these things, I mean, they will be tools to people and the people could, some of the population is like, they're acting on behalf of people or something like that. So maybe those people are in control, but maybe it's a loss of control overall for society in the sense of like outcomes we want or something like that, where you have entities acting on behalf of individuals that are still kind of roughly seen as out of control. Yeah, yeah. This is a question I should have asked earlier. So we were talking about how currently it feels like when you're doing AI engineering or AI research, these models are more like in the category of compiler rather than in the category of a replacement. At some point, if you have quote unquote AGI, it should be able to do what you do. And do you feel like having a million copies of you in parallel results in some huge speed up of AI progress? Basically, if that does happen, do you expect to see an intelligence explosion? Or even once we have it to reach in, not talking about LLMs today, but real AGI. I guess what I mean is, I do, but it's business as usual, because we're in an intelligence explosion already and have been for decades. And when you look at GDP, it's basically the GDP curve that is an exponential weight at some over so many aspects of the industry. Everything is gradually being automated, has been for hundreds of years. Industrial revolution is automation and some of the physical components and tool building and all this kind of stuff. Compilers are early software automation, so I feel like we've been recursively self-improving and exploding for a long time. Maybe another way to see it is, Earth was a pretty, if you don't look at the biomechanics and so on, it was a pretty boring place, I think, and looked very similar if you just look from space and Earth is spinning and then, we're in the middle of this firecracker event, but we're seeing it in slow motion. But I definitely feel like this has already happened for a very long time. Again, I don't see AI as a distinct technology with respect to what has already been happening for a long time. You think it's gonna continue sort of this hyper-exponential trend? And that's why this was very interesting to me because I was trying to find AI in the GDP for a while. I thought that GDP should go up, but then I looked at some of the other technologies that I thought were very transformative, like maybe computers or mobile phones or et cetera. You can't find them in GDP. GDP is the same exponential. And it's just that even, for example, the early iPhone didn't have the app store and it didn't have a lot of the bells and whistles that the modern iPhone has. And so even though we think of 2008 was it, when iPhone came out as like some major seismic change, it's actually not. Everything is like so spread out and so slowly diffuses that everything ends up being averaged up into the same exponential. And it's the exact same thing with computers. You can't find them in the GDP is like, oh, we have computers now. That's not what happened because it's such slow progression. And with AI, we're gonna see the exact same thing. It's just more automation. It allows us to write different kinds of programs that we couldn't write before, but AI is still fundamentally a program. It's a new kind of computer and a new kind of computing system, but it has all these problems. It's gonna diffuse over time. And it's still gonna add up to the same exponential. And we're still gonna get an exponential that's gonna get extremely vertical. And it's going to be very foreign to live in that kind of an environment. Are you saying that like, what will happen is if you go, if you look at the trend before the industrial revolution to currently, you have a hyper exponential where you go from like 0% growth to then 10,000 years ago, 0.02% growth, and then currently we're at 2% growth. So that's the hyper exponential. And you're saying, if you're charting AI on there, then it's like AI takes you to 20% growth or 200% growth. Or you could be saying, if you look at the last 300 years, what you've been seeing is you have technology after technology, computers, electrification, steam engines, railways, et cetera. But the rate of growth is the exact same, it's 2%. So are you saying the rate of growth will, do I basically, I expect this, the rate of growth has also stayed roughly constant, right? For only the last 200, 300 years, but over the course of human history, it's like exploded, right, it's like gone from like 0% basically to like faster, faster, faster, industrial explosion, 2%. Like basically, I guess what I'm saying is for a while, I tried to find AI or look for AI in like the GDP curve. And I kind of convinced myself that this is false. And that even when people talk about recursive self-improvement and labs and stuff like that, I even don't, this is business as usual, of course it's gonna recursively self-improve and it's been recursively self-improving. Like LLMs allow the engineers to work much more efficiently to build the next round of LLM. And a lot more of the components are being automated and tuned and et cetera. So all the engineers having access to Google search is sort of part of it. All the engineers having an ID, all of them having autocomplete or having cloth code, et cetera, it's all just part of the same speed up of the whole thing. So it's just so smooth. But just to clarify, you're saying that the rate of growth will not change. Like, you know, the intelligence explosion will show up it just enabled us to continue staying on the 2% growth trajectory, just as the internet helped us stay on the 2% growth trajectory. Yeah, my expectation is that it stays the same pattern. Yeah, I mean, just to throw the opposite argument against you, my expectation is that it like blows up because I think true AGI, and I'm not talking about LLM coding bots, I'm talking about like actual, this is like a replacement of a human in a server is qualitatively different from these other productivity improving technologies because it's labor itself, right? I think we're living a very labor constrained world. Like if you talk to any startup founder or any person, you can just be like, okay, what do you need more of? You just like need really talented people. And if you just have billions of extra people who are inventing stuff, integrating themselves, making companies bottoms start to finish, that feels qualitatively different from just like a single technology. It's just sort of like just asking if you like, if you get 10 billion extra people on the planet. Maybe a counterpoint, I mean, number one, I'm actually pretty willing to be convinced one way or another on this point. But I will say, for example, computing is labor. Computing was labor. Computers like a lot of jobs disappeared because computers are automating a bunch of digital information processing that you now don't need a human for. And so computers are labor and that has played out. And self-driving as an example is also like computers doing labor. So like, I guess that's already been playing out. So it's still business as usual. Yeah, I guess you have a machine which is spitting out more things like that at potentially a faster pace. And so we historically have, we have examples of the growth regime changing where like you went from, you know, 0.2% growth to 2% growth. So it seems very plausible to me that like a machine, which is then spitting out the next self-driving car and the next internet and whatever. I mean, I kind of, yeah, I see where it's coming from. At the same time, I do feel like people make this assumption of like, okay, we have God in the box and now it can do everything. And it just won't look like that. It's gonna be able to do some of the things. It's gonna fail at some other things. It's gonna be gradually put into society and basically end up with the same pattern, is my prediction. Because this assumption of suddenly having a completely intelligent, fully flexible, fully general human in a box and we can dispense it at arbitrary problems in society, I don't think that we will have this like discrete change. And so I think we'll arrive at the same kind of a gradual diffusion of this across the industry. I think what often ends up being misleading in these conversations is people, I don't like to use the word intelligence in this context because intelligence implies you think like, oh, superintelligence will be sitting, there'll be a single superintelligence sitting in a server and it'll like divine how to come up with new technologies and inventions that causes this explosion. And that's not what I'm imagining, what I'm imagining 20% growth. I'm imagining that there's billions of, basically like very smart human like minds potentially, or that's all that's required. But the fact that there's hundreds of millions of them, billions of them, each individually making new products, figuring out how to integrate themselves into the economy, just the way if like a highly experienced, smart immigrant came to the country, you wouldn't need to like figure out how we integrate them in the economy. They figure it out, they could start a company, they could like make inventions, or like just increase productivity in the world. And we have examples even in the current regime of places that have had 10, 20% economic growth. If you just have a lot of people and less capital in comparison to the people, you can have Hong Kong or Shenzhen or whatever, just had decades of 10% plus growth. And I think there's a lot of really smart people who are ready to like make use of the resources and do this like period of catch up because we've had this discontinuity. And I think, yeah, it might be similar. I think I understand, but I still think that you're presupposing some discrete jump, there's some unlock that we're waiting to claim. And suddenly we're gonna have geniuses in data centers. And I still think you're presupposing some discrete jump that I think has basically no historical precedent that I can't find in any of the statistics and that I think probably won't happen. I mean, the initial revolution is such a jump, right? You went from like 0.2% growth to 2% growth. I'm just saying like you'll see another jump like that. I'm a little bit suspicious, I would have to look at it. I'm a little bit suspicious and I would have to take a look. For example, like maybe some of the logs are not very good from before the industrial revolution or something like that. So I'm a little bit suspicious of it, but yeah, maybe you're right. I don't have strong opinions. Maybe you're saying that this was a singular event that was extremely magical. And you're saying that maybe there's gonna be another event that's gonna be just like that, extremely magical. It will break paradigm and so on. I actually don't think, I mean, the crucial thing with the industrial revolution was that it was not magical, right? Like if you just zoomed in, what you would see in 1770 or 1870 is not that there was some key invention. Yeah, exactly. But at the same time, you did move the economy to a regime where the progress was much faster and the exponential 10x'd. And I expect a similar thing from AI where it's not like there's gonna be a single moment where we made the crucial invention. There's some overhang that's being unlocked. Like maybe there's a new energy source. There's some unlock in this case, some kind of a cognitive capacity and there's an overhang of cognitive work to do. That's right. And you're expecting that overhang to be filled by this new technology when it crosses the threshold. Yeah, and I mean, maybe one way to think about it is through history, a lot of growth, I mean, growth comes because people come up with ideas and then people are like out there doing stuff to execute those ideas and make valuable output. And through most of this time, population isn't exploding that has been driving growth. For the last 50 years, people have argued that growth has stagnated. Population in frontier countries has also stagnated. I think we go back on the hyper exponential growth in population and output. Sorry, exponential growth in population that causes hyper exponential growth and output. Yeah, I mean, yeah, it's really hard to tell. I understand that viewpoint. I don't intuitively feel that viewpoint. So we just got access to Google's VO 3.1 and it's been really cool to play around with. The first thing we did was run a bunch of prompts through both VO3 and 3.1 to see what's changed in the new version. So here's VO3. Hi, I'm Max and I got stuck in a local minimum again. It's okay, Max, we've all been there. Took me three epochs to get out. And here's VO3.1. Hi, I'm Max and I got stuck in a local minimum again. It's okay, Max, we've all been there. Took me three epochs to get out. 3.1's output is just consistently more coherent and the audio is noticeably higher quality. We've been using VO for a while now, actually. We released an essay earlier this year about AI firms fully animated by VO2. And it's been amazing to see how fast these models are improving. This update makes VO even more useful in terms of animating our ideas and our explainers. You can try VO right now in the Gemini app with pro and ultra subscriptions. You can also access it through the Gemini API or through Google Flow. You recommended Nick Lane's book to me and then on that basis, I also found it super interesting and I interviewed him. And so I actually have some questions about sort of thinking about intelligence and evolutionary history. Now that you, over the last 20 years of doing AI research, you maybe have a more tangible sense of what intelligence is, what it takes to develop it. Are you more or less surprised as a result that evolution just sort of spontaneously stumbled upon it? I love Nick Lane's books, by the way. So yeah, I was just listening to his podcast on the way up here. With respect to intelligence and its evolution, I do think it came fairly, I mean, it's very, very recent, right? I am surprised that it evolved, yeah. I find it fascinating to think about all the worlds out there, like say there's a thousand planets like Earth and what they look like. I think Nick Lane was here talking about some of the early parts, right? Like, okay, he expects basically very similar life forms, roughly speaking, and bacteria-like things in most of them. And then there's a few breaks in there. I would expect that the evolution of intelligence intuitively feels to me like it should be a fairly rare event and there have been animals for, I guess maybe you should base it on how long something has existed. So for example, if bacteria have been around for two billion years and nothing happened, then going to your carrier, it's probably pretty hard because bacteria actually came up quite early in Earth's evolution or history. And so I guess how long have we had animals, maybe a couple hundred million years, like multicellular animals that like run, run, crawl, et cetera, which is maybe 10% of Earth's lifespan or something like that. Maybe on that time scale it's actually not too tricky. I still feel like it's still surprising to me, I think intuitively, that it developed. I would maybe expect just a lot of animal-like life forms doing animal-like things. The fact that you can get something that creates culture and knowledge and accumulates it is surprising to me. So there's actually a couple of interesting follow-ups. If you buy this sudden perspective that actually the crux of intelligence is animal intelligence, what the quote he said is, if you got to the squirrel, you'd be most of the way to AGI. Then we got to squirrel intelligence, I guess, right after the Cambrian explosion, 600 million years ago. It seems like what instigated that was the oxygenation event 600 million years ago, but immediately, the sort of like intelligence algorithm was there to like make the squirrel intelligence, right? So it's suggestive that animal intelligence was like that. As soon as you had the oxygen in the environment, you had the ecuriot, you could just like get the algorithm. Maybe there was like sort of an accident that evolution smelled a little bit on it so fast, but I don't know if that suggests it's actually quite, at the end, gonna be quite simple. Yes, basically it's so hard to tell, right, with any of this stuff. I guess you can base it a little bit on how long something has existed or how long it feels like something has been bottlenecked. So Nicolay is very good about describing this like very apparent bottleneck in bacteria and archaea for two billion years, nothing happened. Like extreme diversity of chemical, of biochemistry, and yet nothing that grows to become animals for two billion years. I don't know that we've seen exactly that kind of an equivalent with animals and intelligence, to your point, right? But I guess maybe we could also look at it with respect to how many times we think evolution, sorry, intelligence has like individually sprung up. That's a really good thing to investigate. Maybe one thought on that is I almost feel like, well, there's the hominid intelligence, and then there's, I would say, like the bird intelligence, right, like ravens, et cetera, are extremely clever, but their brain parts are actually quite distinct and we don't have that much existence. So maybe that's a slight event of, there's a slight indication of maybe intelligence springing up a few times. And so in that case, you'd maybe expect it more frequently or something like that. Yeah, a former guest, Gwern, and also Carl Schumann have made a really interesting point about that, which is their perspective is that the scalable algorithm which humans have and primates have arose in birds as well, and maybe other times as well. But humans found a evolutionary niche, which rewarded marginal increases in intelligence, and also had a scalable brain algorithm that could achieve those increases in intelligence. And so, for example, if a bird had a bigger brain, it would just like collapse out of the air. So it's very smart for the size of its brain, but it's like, it's not in a niche which rewards the brain getting bigger. Maybe similar with some really smart. Dolphins, et cetera. Exactly, yeah. Whereas humans, we have hands that reward being able to learn how to do tool use, we can externalize digestion, more energy to the brain, and that kicks off the flywheel. Oh yeah, and just stuff to work with. I mean, I'm guessing it would be harder to, if I was a dolphin, I mean, how do you do, you can't have fire, for example, and stuff like that. I mean, they're probably like the universe of things you can do in water, like inside water, is probably lower than what you can do on land, just chemically. Yeah, I do agree with this viewpoint of these niches and what's being incentivized. I still find it kind of miraculous that, I would have maybe expected things to get stuck on animals with bigger muscles, you know? Like going through intelligence is actually a really fascinating breaking point. The way we're imprinted is, the reason it was so hard is, there is a very tight line between being in a situation where something is so important to learn, that it's not just worth distilling the exact right circuits directly back into your DNA, versus it's not important enough to learn at all. It has to be something which is like, you have to incentivize building the algorithm to learn in lifetime. Yeah, exactly. You have to incentivize some kind of adaptability. You actually want something that, you actually want environments that are unpredictable. So, evolution can't bake your algorithms into your weights. A lot of animals are basically pre-baked in this sense. And so, humans have to figure it out at test time when they get born. And so, maybe there was, you actually want these kinds of environments that actually change really rapidly or something like that, where you can't foresee what will work well. And so, you actually put all that intelligence, you create intelligence to figure it out at test time. So, Quentin Pope had this interesting blog post where he's saying, the reason he doesn't expect a sharp takeoff is, so humans had the sharp takeoff, where 60,000 years ago, we seem to have had the cognitive architectures that we have today. And 10,000 years ago, agricultural revolution, modernity, dot, dot, dot. What was happening in that 50,000 years? Well, you had to build this sort of cultural scaffold where you can accumulate knowledge over generations. This is an ability that exists for free in the way we do AI training, where if you retrain a model, it can still, I mean, in many cases, they're literally distilled, but they can be trained on each other. They can be trained on the same pre-training corpus. They don't literally have to start from scratch. So, there's a sense in which the thing which, it took humans a long time to get this cultural loop going, just comes for free with the way we do LLM training. Yes and no, because LLMs don't really have the equivalent of culture. And maybe we're giving them way too much and incentivizing not to create it or something like that. But I guess like the invention of culture and of written record and of like passing down notes between each other, I don't think there's an equivalent of that with LLMs right now. So, LLMs don't really have culture right now. And it's kind of like one of the, I think impediments, I would say. Can you give me some sense of what LLM culture might look like? So, in the simplest case, it would be a giant scratch pad that the LLM can edit. And as it's reading stuff or as it's helping out with work, it's editing the scratch pad for itself. Why can't an LLM write a book for the other LLMs? That would be cool. Like, why can't other LLMs read this LLMs book and be inspired by it or shocked by it or something like that? There's no equivalence for any of this stuff. Interesting. When would you expect that kind of thing to start happening? And more general question about like multi-agent systems and a sort of like independent AI civilization and culture. I think there's two powerful ideas in the realm of multi-agent that have both not been like really claimed or so on. The first one I would say is culture. And LLMs basically are growing a repertoire of knowledge for their own purposes. The second one looks a lot more like the powerful idea of self-play, in my mind, is extremely powerful. So, evolution actually has a lot of competition, basically driving intelligence and evolution. And in AlphaGo, more algorithmically, like AlphaGo is playing against itself and that's how it learns to get really good at Go. And there's no equivalent of self-playing LLMs, but I would expect that to also exist, but no one has done it yet. Like, why can't an LLM, for example, create a bunch of problems that another LLM is learning to solve? And then the LLM is always trying to like serve more and more difficult problems, stuff like that. So, like, I think there's a bunch of ways to actually organize it. And I think it's a realm of research. But I think I haven't seen anything that convincingly claims both of those, like multi-agent improvements. I still think we're mostly in the realm of a single individual agent, but I also think that will change. And in the realm of culture also, I would bucket also organizations. And we haven't seen anything like that convincingly either. So, that's why we're still early. And can you identify the key bottleneck that's preventing this kind of collaboration between LLMs? Maybe like the way I would put it is, somehow remarkably, again, some of these analogies work and they shouldn't, but somehow remarkably they do. A lot of the smaller models, or the smaller models somehow remarkably resemble like a kindergarten student, or then like a elementary school student, or high school student, et cetera. And somehow we still haven't graduated enough where the stuff can take over. Like it's still mostly like my cloth code or codex, they still kind of feel like this elementary grade student. I know that they can take PhD quizzes, but they still cognitively feel like a kindergartener and elementary school student. So, I don't think they can create culture because they're still kids. Like they're savant kids. They have episodic, they have perfect memory of all this stuff, et cetera. And they can convincingly create all kinds of slop that looks really good. But I still think they don't really know what they're doing and they don't really have the cognition across all these little checkboxes that we still have to collect. Yeah. So, you've talked about how you were at Tesla leading self-driving from 2017 to 2022. And then you firsthand saw this progress from, we went from cool demos to now thousands of cars out there actually autonomously doing drives. Why did that take a decade? Like what was happening through that time? Yeah. So, I would say one thing I will almost instantly also push back on is this is not even near done. So, in a bunch of ways that I'm gonna get to. I do think that self-driving is very interesting because it's definitely like where I get a lot of my intuitions because I spent five years on it. And it has this entire history where actually the first demos of self-driving go all the way to 1980s. You can see a demo from CMU in 1986. There's a truck that's driving itself on roads. But okay, fast forward. I think when I was joining Tesla, I had a very early demo of a Waymo and it basically gave me a perfect drive in 2014 or something like that. So, perfect Waymo drive a decade ago. Took us around Palo Alto and so on because I had a friend who worked there. And I thought it was like very close and then still took a long time. And I do think that for some kinds of tasks and jobs and so on, there's a very large demo to product gap where the demo is very easy but the products are very hard. And it's especially the case in cases like self-driving where the cost of failure is too high, right? Many industries, tasks and jobs maybe don't have that property. But when you do have that property that definitely increases the timelines. I do think that for example, in software engineering, I do actually think that that property does exist. I think for a lot of vibe coding, it doesn't. But I think if you're writing actual production grade code, I think that property should exist because any kind of mistake actually leads to a security vulnerability or something like that. And millions and hundreds of millions of people's personal social security numbers, et cetera, get leaked or something like that. And so I do think that it is a case that in software, people should be careful. Kind of like in self-driving. Like in self-driving, if things go wrong, you might get injury in, I guess there's worse outcomes. But I guess in software, I almost feel like it's almost unbounded how terrible something could be. What you're saying? So I do think that they share that property. And then I think basically what takes the long amount of time and the way to think about it is that it's a march of nines and every single nine is a constant amount of work. So every single nine is the same amount of work. So when you get a demo and something works 90% of the time, that's just the first nine. And then you need a second nine, a third nine, fourth nine, fifth nine. And while I was at Tesla for, was it five years or so, I think we went through maybe three nines, two nines, I don't know what it is. But like multiple nines of iteration, there's still more nines to go. And so that's why these things take so long. And so it's definitely formative for me, like seeing something that was a demo. I'm very unimpressed by demos. So whenever I see demos of anything, I'm extremely unimpressed by that. It works better if you can, if it's a demo that someone cooked up and is just showing you it's worst. If you can interact with it, it's a bit better. But even then you're not done, you need actual product. It's gonna face all these challenges when it comes in contact with reality and all these different pockets of behavior that need patching. And so I think we're gonna see all this stuff play out. It's a march of nines, each nine is constant. Demos are encouraging, still a huge amount of work to do. I do think it is a kind of a critical safety domain, unless you're doing byte coding, which is all nice and fun and so on. And so that's why I think this also enforced my timelines from that perspective. That's very interesting to hear you say that the sort of safety guarantees you need from software are actually not dissimilar to self-driving because what people will often say is that self-driving took so long because the cost of failure is so high. Like a human makes a mistake on average every 400,000 miles or every seven years. And if you had to release a coding agent that couldn't make a mistake for at least seven years, it would be much harder to deploy. But I guess your point is that if you made a catastrophic coding mistake, like breaking some important system every seven years. It's very easy to do. And in fact, in terms of sort of wall clock time, it would be much less than seven years because you're constantly outputting code like that. So like per tokens, in terms of tokens, it would be seven years, but in terms of wall clock time, it would be pretty close. Yeah, in some ways it's a much harder problem. I mean, self-driving is just one of thousands of things that people do. It's almost like a single vertical, I suppose. Whereas when we're talking about general software engineering, it's even more, there's more surface area. There's another objection people make to that analogy, which is that with self-driving, what took a big fraction of that time was solving the problem of having basic perception that's robust and building representations and having a model that has some common sense so it can generalize to when it sees something that's slightly out of distribution. If somebody's waving down the road this way, you don't need to train for it. The thing will have some understanding of how to respond to something like that. And these are things we're getting for free with LLMs or VLMs today. So we don't have to solve these very basic representation problems. And so now deploying AIs across different domains will sort of be like deploying a self-driving car with current models to a different city, which is hard, but not like a 10 year long task. Yeah, basically I'm not 100% sure if I fully agree with that. I don't know how much we're getting for free. And I still think there's like a lot of gaps in understanding in what we are getting. I mean, we're definitely getting more generalizable intelligence in a single entity. Whereas self-driving is a very special purpose task that requires, in some sense, building a special purpose task is maybe even harder in a certain sense because it doesn't like fall out from a more general thing that you're doing at scale, if that makes sense. So, but I still think that the analogy doesn't, I still don't know if it fully resonates because like the LLMs are still pretty fallible and I still think that they have a lot of gaps and that it still needs to be filled in. And I don't think that we're getting like magical generalization completely out of the box sort of in a certain sense. And the other aspect that I want to also actually return to when I was in the beginning was self-driving cars are nowhere in their down still. So even though, so the deployments still are pretty minimal, right? So even Waymo and so on has very few cars and they're doing that roughly speaking because they're not economical, right? Because they've built something that lives in the future. And so they had to like pull back future, but they had to make it uneconomical. So they have all these like, you know, there's all these costs, not just marginal costs for those cars and their operation and maintenance, but also the capex of the entire thing. So making economical is still gonna be a slog, I think for them. And then also I think when you look at these cars and there's no one driving, I also think it's a little bit deceiving because there are actually very elaborate teleoperation centers of people actually kind of like in a loop with these cars. And I don't have the full extent of it, but I think there's more human in the loop that you might expect. And there's people somewhere out there basically beaming in from the sky. And I don't actually know that they're fully in the loop with the driving. I think some of the times they are, but they're certainly involved in their people. And in some sense, we haven't actually removed the person. We've like moved them to somewhere where we can't see them. I still think there will be some work, as you mentioned, going from environment to environment. And so I think like there's still challenges to make self-driving real, but I do agree that it's definitely across the threshold where it kind of feels real, unless it's like really teleoperated. For example, Waymo can't go to all the different parts of the city. My suspicion is it's like parts of city where you don't get good signal. Anyway, so basically I don't actually know anything about the stack. I mean, I'm just making up stuff. You let self-driving for five years at Tesla. Sorry, I didn't know anything about the specifics of Waymo. I feel like I talked about them. I actually, by the way, love the Waymo and I take it all the time. So I don't want to say like, I just think that people again are sometimes a little bit too naive about some of the progress. And I still think there's a huge amount of work. And I think Tesla took in my mind, a lot more scalable approach. And I think the team is doing extremely well and it's gonna, and I'm kind of like on the record for predicting how this will go, which is like Waymo had like early start because you can package up so many sensors. But I do think Tesla is taking the more scalable strategy and it's gonna look a lot more like that. So I think this will have to still play out and hasn't. But basically like, I don't want to talk about self-driving as something that took a decade because it didn't take. It didn't take yet. If that makes sense. Because one, it's the start is at 1980, not 10 years ago. And then two, the end is not here yet. Yeah, the end is not near yet because when we're talking about self-driving, usually in my mind it's self-driving at scale. Yeah. People don't have to get a driver's license, et cetera. I'm curious to bounce to other ways in which the analogy might be different. And the reason I'm especially curious about this is because I think the question of how fast AI is deployed, how valuable it is when it's early on is potentially the most important question in the world right now. If you're trying to model what the year or 20 or 30 looks like, this is the question you want to have some understanding of. So another thing you might think is, one, you have this latency requirement with self-driving where you have, I have no idea what the actual models are, but I assume like tens of millions of parameters or something, which is not the necessary constraint for knowledge work with LLMs. Or maybe it might be with the computer use and stuff. But anyways, the other big one is, maybe more importantly, on this CapEx question, yes, there is additional cost to serving up an additional copy of a model, but the sort of OPEX of a session is quite low and you can amortize the cost of AI into the training run itself, depending on how inference scaling goes and stuff. But it's certainly not as much as like building a whole new car to serve another instance of a model. So it just, the economics of deploying more widely are much more favorable. I think that's right. I think if you're sticking in a realm of bits, bits are like a million times easier than anything that touches the physical world. I definitely grant that. Bits are completely changeable, arbitrarily reshuffleable at very rapid speed. So you would expect a lot more faster adaptation also in the industry and so on. And then what was the first one? The latency requirements. Oh, the latency requirements. And its implications for model size. I mean, I also think that if we are talking about knowledge work at scale, there will be some latency requirements practically speaking because we're gonna have to make, create a huge amount of compute and serve that. And then I think like the last aspect that I very briefly wanna also talk about is like all the rest of it, just all the rest of it. So what does society think about it? What is the legal, how is it working legally? How is it working insurance wise? Who's really, like what are those layers of it and aspects of it? What happens with, what is the equivalent of people putting a cone on a Waymo? Yeah. There's gonna be equivalents of all that. And so I do think that, I almost feel like self-driving is a very nice analogy that you can borrow things from. Yeah, what is the equivalent of a cone on a car? What is the equivalent of a tele-operating worker who's like hidden away? Mm-hmm. And almost like all the aspects of it. Yeah, do you have any opinions on whether this implies that the current AI build out, which would like 10X the amount of available computer in the world in a year or two and maybe like 100, more than 100 exit by the end of the decade. If the use of AI will be lower than some people naively predict, does that mean that we're overbuilding compute or is that a separate question? Kind of like what happened with railroads and all this kind of stuff. What, sorry? Was it railroads? Oh, sorry, it was, yeah. There is like a circle precedent or was it with telecommunication industry, right? Like prepaving the internet that only came like a decade later, you know, and creating like a whole bubble in the telecommunications industry in the late 90s kind of thing, yeah. So I don't know. I mean, I understand I'm sounding very pessimistic here. I'm only doing that, I'm actually optimistic. I think this will work. I think it's tractable. I'm only sounding pessimistic because when I go on my Twitter timeline, I see all this stuff that makes no sense to me. And I think there's a lot of reasons for why that exists. And I think a lot of it is, I think honestly, just a fundraising, it's just incentive structures. A lot of it may be fundraising. A lot of it is just attention, you know, converting attention to money on the internet, you know, stuff like that. So I think there's a lot of that going on and I think I'm only reacting to that, but I'm still like overall very bullish on technology. I think we're gonna work through all this stuff. And I think there's been a rapid amount of progress. I don't actually know that there's overbuilding. I think that there's gonna be, we're gonna be able to gobble up what in my understanding is being built because I do think that, for example, cloud code or OpenAI codecs and stuff like that, they didn't even exist a year ago, right? Is that right? I think it's roughly right. This is miraculous technology that didn't exist. I think there's gonna be a huge amount of demand as we see the demand in charge of PT already and so on. So yeah, I don't actually know that there's overbuilding, but I guess I'm just reacting to like some of the very fast timelines that people continue to say incorrectly and I've heard many, many times over the course of my 15 years in AI where very reputable people keep getting this wrong all the time. And I think I want us to be properly calibrated and I think some of this also, it does have like geopolitical ramifications and things like that when, like some of these questions. And I think I don't want people to make mistakes on that sphere of things. So I do want us to be grounded in reality of what technology is and isn't. So. Let's talk about education in Eureka and stuff. One thing you could do is start another AI lab and try to solve those problems. Yeah, if you're curious, what are you up to now? And then, yeah, why not AI research itself? I guess maybe like the way I would put it is I feel some amount of like determinism around the things that AI labs are doing. And I feel like I could help out there, but I don't know that I would like uniquely, I don't know that I would like uniquely improve it, but I think like my personal big fear is that a lot of this stuff happens on the side of humanity and that humanity gets disempowered by it. And I kind of like, I care not just about all the Dyson spheres that we're gonna build and that AI is gonna build in a fully autonomous way, I care about what happens to humans. And I want humans to be well off in this future. And I feel like that's where I can a lot more uniquely at value than like an incremental improvement in the Frontier lab. And so I guess I'm most afraid of something maybe like, depicted in movies like WALL-E or Idiocracy or something like that where humanity is sort of on the side of this stuff. And I want humans to be much, much better in this future. And so I guess to me, this is kind of like through education that you can actually achieve this. And so what are you working on there? Oh yeah, so Eureka is trying to build, I think maybe the easiest way I can describe it is we're trying to build the Starfleet Academy. I don't know if you've watched Star Trek. I haven't. Okay. Okay, Starfleet Academy is this like elite institution for Frontier technology, building spaceships and graduating cadets to be like in the pilots of these spaceships and whatnot. So I just imagine like an elite institution for technical knowledge and basically a kind of school that's very up to date and very like a premier institution. A category of questions I have for you is just explaining how one teaches technical or scientific content well, because you are one of the world masters at it. And then I'm curious both about how you think about it for content you've already put out there on YouTube. But also to the extent it's any different, how you think about it for Eureka? Yeah, with respect to Eureka, I think like one thing that is very fascinating to me about the education is like, I do think education will pretty fundamentally change with AIs on the side. And I think it has to be rewired and changed to some extent. I still think that we're pretty early. I think there's gonna be a lot of people who are gonna try to do the obvious things, which is like, oh, have an LLM and ask it questions and do all the basic things that you would do via prompting right now. I think it's helpful, but it still feels to me a bit like slop. I'd like to do it properly and I think the capability is not there for what I would want. What I'd want is like an actual tutor experience. Maybe a prominent example in my mind is I was recently learning Korean, some language learning. And I went through a phase where I was learning Korean by myself on the internet. I went through a phase where I was actually part of a small class in Korea, taking a Korean with a bunch of other people, which was really funny. But we had a teacher and like 10 people or so taking Korean. And then I switched to a one-on-one tutor. And I guess what was fascinating to me is I think I had a really good tutor, but I mean, just thinking through like what this tutor was doing for me and how incredible that experience was and how high the bar is for like what I actually want to build eventually. Because I mean, she was extremely, so she instantly from a very short conversation understood like where I am as a student, what I know and don't know. And she was able to like probe exactly like the kinds of questions or things to understand my world model. No LLM will do that for you 100% right now, not even close, right? But a tutor will do that if they're good. Once she understands, she actually like really served me all the things that I needed at my current sliver of capability. I need to be always appropriately challenged. I can't be faced with something too hard or too trivial. And a tutor is really good at serving you just the right stuff. And so basically I felt like I was the only constraint to learning like my own, I was the only constraint. I was always given the perfect information. I'm the only constraint. And I felt good because I'm the only impediment that exists. It's not that I can't find knowledge or there's not properly explained or et cetera. Like it's just my ability to memorize and so on. And this is what I want for people. How do you automate that? So very good question. At the current capability, you don't. But I do think that with as, and that's why I think it's not actually the right time to actually build this kind of an AI tutor. I still think it's a useful product and lots of people will build it. But I still feel like the bar is so high and the capability is not there. But I mean, even today I would say Charter BT is an extremely valuable educational product. But I think for me it was so fascinating to see how high the bar is. And when I was with her, I almost felt like there's no way I can build this. But you are building it, right? Anyone who's had a really good tutor is like, how are you gonna build this? So I guess I'm waiting for that capability. I do think that in a lot of ways in the industry, for example, I did some AI consulting for computer vision. A lot of my times, the value that I brought to the company was telling them not to use AI. It wasn't like I was the AI expert and they described the problem and I said, don't use AI. This was my value add. And I feel like it's the same in education right now where I kind of feel like for what I have in mind, it's not yet the time, but the time will come. But for now I'm building something that looks maybe a bit more conventional, that has a physical and digital component and so on. But I think it's obvious how this should look like in the future. Do they send you a really nice day? What is the thing you hope will be released this year or next year? Well, so I'm building the first course and I want to have a really, really good course. State of the art, obvious state of the art destination you go to learn AI in this case, because that's just what I'm familiar with. So I think it's a really good first product to get to be really good. And so that's what I'm building and NanoChat, which you briefly mentioned is a capstone project of LLM101N, which is a class that I'm building. So that's a really big piece of it, but now I have to build out a lot of the intermediates and then I have to actually hire a small team of TAs and so on and actually build the entire course. And maybe one more thing that I would say is like, many times when people think about education, they think about sort of like the more, what I would say is like kind of a softer component of diffusing knowledge or like, but I actually have something very hard and technical in mind. And so in my mind, education is kind of like the very difficult technical like process of building ramps to knowledge. So in my mind, NanoChat is a ramp to knowledge because it's a very simple, it's like the super simplified full stack thing. If you give this artifact to someone and they like look through it, they're learning a ton of stuff. And so it's giving you a lot of what I call eurekas per second, which is like understanding per second. That's what I want, lots of eurekas per second. And so to me, this is a technical problem of how do we build these ramps to knowledge? And so I always think of eureka as almost like, it's not like maybe that different maybe through some of the frontier labs or some of the work that's gonna be going on because I want to figure out how to build these frontier, these ramps very efficiently so that people are never stuck and everything is always not too hard or not too trivial. And you have just the right material to actually progress. Yeah, so you're imagining the short term that instead of a tutor being able to like probe your understanding, if you have enough self-awareness to be able to probe yourself, you're never gonna be stuck. You can like find the right answer between talking to the TA or talking to another one and looking at the reference implementation. It sounds like automation or AI is actually not as good even. Like so far it's actually the big alpha here is your ability to explain AI codified in the source material of the class, right? That's like fundamentally what the course is. I mean, I think you always have to be calibrated to what the capability exists in the industry. And I think a lot of people are gonna pursue like, oh, just ask ChachiPT, et cetera. But I think like right now, for example, if you go to ChachiPT and you say, oh, teach me AI, there's no way, it's gonna give you some slop, right? Like AI is never gonna write nano chat right now, but nano chat is a really useful, I think, intermediate point. So I'm collaborating with AI to create all this material. So AI is still fundamentally very helpful. Earlier on, I built a CS231N at Stanford, which was one of the earlier, actually sorry, I think it was the first deep learning class at Stanford, which became very popular. And the difference in building out 231N and LLM101N now is quite stark because I feel really empowered by the LLMs as they exist right now, but I'm very much in the loop. So they're helping me build little materials, I go much faster, they're doing a lot of boring stuff, et cetera. So I feel like I'm developing the course much faster and those LLM infused in it, but it's not yet at a place where I can creatively create the content, I'm still there to do that. So I think the trickiness is always calibrating yourself to what exists. And so when you imagine what is available through Eureka in a couple of years, it seems like the big bottleneck is gonna be finding corpothes in field after field who can convert their understanding into these ramps, right? So I think it would change over time. So I think right now it would be hiring faculty to help work hand in hand with AI and a team of people probably to build a state of the art courses. And then I think over time it can, maybe some of the TAs can actually become AIs because some of the TAs like, okay, you just take all the course materials and then I think you could serve a very good like a real automated TA for the student when they have more basic questions or something like that, right? But I think you'll need faculty for the overall architecture of a course and making sure that it fits. And so I kind of see a progression of how this will evolve and maybe at some future point, I'm not even that useful in AI is doing most of the design much better than I could. But I still think that that's gonna take some time to play out. But are you imagining that like people who have expertise in other fields are then contributing courses or do you feel like it's actually quite essential to the vision that you, given your understanding of how you want to teach are the one designing the content? Like, I don't know, Sal Khan is like narrating all the videos of Khan Academy. Yeah. Are you imagining something like that or? Oh no, I will hire faculty I think because there are domains in which I'm not an expert. And I think that's the only way to offer the state of the art experience for the student ultimately. So yeah, I do expect that I would hire faculty, but I will probably stick around in AI for some time. But I do have something I think more conventional in mind for the current capability. I think that what people would probably anticipate. And when I'm building Starfield Academy, I do probably imagine a physical institution and maybe a tier below that, a digital offering that is not the state of the art experience you would get when someone comes in physically full-time and we work through material from start to end and make sure you understand it. That's the physical offering. The digital offering is yeah, a bunch of stuff on the internet and maybe some LLM assistant and it's a bit more gimmicky in a tier below, but at least it's accessible to like 8 billion people. Yeah, I think you're basically inventing college from first principles for the tools that are available today and then just like selecting for people who have the motivation and the interest of actually really engaging with material. Yeah, and I think there's gonna have to be a lot of not just education, but also re-education. And I would love to help out there because I think the jobs will probably change quite a bit. And so for example, today a lot of people are trying to upskill in AI specifically. So I think it's a really good course to teach in this respect. And yeah, I think the motivation-wise before AGI, motivation is very simple to solve because people want to make money and this is how you make money in the industry today. I think post-AGI is a lot more interesting possibly because yeah, if everything is automated and there's nothing to do for anyone, why would anyone go to a school, et cetera? So I think, I guess like I often say that pre-AGI education is useful, post-AGI education is fun. And in a similar way as people, for example, people go to gym today, but we don't need their physical strength to manipulate heavy objects because we have machines that do that. So they still go to gym, why do they go to gym? Well, because it's fun, it's healthy, and you look hot when you have a six pack, I don't know. I guess what I'm saying is it's attractive for people to do that in a certain very deep psychological evolutionary sense for humanity. And so I kind of think that education will kind of play out in the same way, like you'll go to school, like you go to gym. And I think that right now, I think not that many people learn because learning is hard. You bounce from material, and some people overcome that barrier, but for most people it's hard. But I do think that it's a technical problem to solve. It's a technical problem to do what my tutor did for me when I was learning Korean. I think it's tractable and buildable, and someone should build it. And I think it's gonna make learning anything trivial and desirable, and people will do it for fun because it's trivial. If I had a tutor like that for any arbitrary piece of knowledge, I think it's gonna be so much easier to learn anything, and people will do it. And they'll do it for the same reasons they go to gym. I mean, that sounds different from using this, as opposed to AGI, you're using this to basically as entertainment or as like self-betterment. But it sounded like you had a vision also that this education is relevant to keeping humanity in control of AI. And they sound different, and I'm curious, is it like it's entertaining for some people, but then empowerment for some others? How do you think about that? So I do definitely feel like people will be, I do think like eventually it's a bit of a losing game, if that makes sense. I do think that it is in long-term, long-term, which I think is longer than I think maybe most people in the industry, it's a losing game. I do think that people can go so far, and that we barely scratched the surface of how much a person can go. And that's just because people are bouncing off of material that's too easy or too hard. And I actually kind of feel that people will be able to go much further. Like anyone speaks five languages, because why not, because it's so trivial. Anyone knows all the basic curriculum of underground, et cetera. Now that I'm understanding the vision, that's very interesting. Like I think it actually has a perfect analog in gym culture. I don't think 100 years ago, anybody would be like ripped. Like nobody would have, you know, be able to like just spontaneously bench two plays or three plays or something. And it's actually very common now. Because this idea of systematically training and lifting weights in the gym or systematically training to be able to run a marathon, which is a capability spontaneously you would not have or most humans would not have. And you're imagining similar things for learning across many different domains, much more intensely, deeply, faster. Yeah, exactly. And I kind of feel like I am betting a little bit implicitly on some of the timelessness of human nature. And I think it will be desirable to be, to do all these things. And I think people will look up to it as they have for millennia, because, and I think this will continue to be true. And actually also maybe there's some evidence of that historically, because if you look at, for example, aristocrats, or you look at maybe ancient Greece or something like that, whenever you had little pocket environments that were post-AGI in a certain sense, I do feel like people have spent a lot of their time flourishing in a certain way, either physically or cognitively. And so I think I feel okay about the prospects of that. And I think if this is false and I'm wrong, and we end up in Wally or a radiocracy future, then I think it's very, I don't even care if there's like Dyson spheres, this is a terrible outcome. I actually really do care about humanity. Everyone has to just be superhuman in a certain sense. I guess it's still a world in which that is not enabling us to, it's like the culture world, right? You're not fundamentally gonna be able to transform the trajectory of technology or influence decisions by your own labor or cognition alone. Maybe you can influence decisions because the AI is like for your approval, but you're not like, it's not because I've like, I can, because I've invented something or I've like come up with a new design, I'm like really influencing the future. Yeah, maybe. I don't actually think that, I think there will be transitionary period where we are gonna be able to be in the loop and advance things if we actually understand a lot of stuff. I do think that long-term that probably goes away, right? But maybe it's gonna become a sport. Like right now you have power lifters who go extreme on this direction. So what is power lifting in a cognitive era? Maybe it's people who are really trying to make Olympics out of knowing stuff. And if you have a perfect AI tutor, maybe you can get extremely far. I almost feel like we're just barely, the geniuses of today are bare sketch on the surface of what a human mind can do, I think. I love this vision. I also, I feel like the person who I have most product market fit with is like me because my job involves having to learn different subjects every week. And I am like very excited if you can. I'm similar for that matter. I mean, a lot of people, for example, hate school and wanna get out of it. I really liked school, I loved learning things, et cetera. I wanted to stay in school. I stayed all the way until PhD and then they wouldn't let me stay longer. So I went to the industry. But I mean, basically, roughly speaking, I love learning, even for the sake of learning, but I also love learning because it's a form of empowerment and being useful and productive. I think you also made a point that was subtle. So just to spell it out, I think what's happened so far with online courses is that why haven't they already enabled us to, enabled every single human to know everything. And I think they're just so motivation-laden because there's not obvious on-ramps and it's so easy to get stuck. And if you had instead this thing, basically like a really good human tutor, it would just be such an unlock from a motivation perspective. I think so. Because it feels bad to bounce from material. It feels bad. You get negative reward from a sinking amount of time in something and it doesn't pan out or being completely bored because of which you're getting us too easy or too hard. So I think, yeah, I think when you actually do it properly, learning feels good. Yeah. It's a technical problem to get there. And I think for a while it's gonna be AI plus human collab. And at some point maybe it's just AI. Can I ask some questions about teaching well? If you had to like sort of like give advice to another educator in another field that you're curious about to make the kinds of YouTube tutorials you've made. Maybe it might be especially interesting to talk about domains where you can't just like, you can't test somebody's technical understanding by having them code something up or something. What advice would you give them? So I think that's a pretty broad topic. I do feel like there's basically, I almost feel like there are 10, 20 tips and tricks that I kind of semi-consciously probably do. But I guess like on a high level, I always try to, I think a lot of this comes from my physics background. I really, really did enjoy my physics background. I have a whole rant on I think how everyone should learn physics in early school education. Because I think early school education is not about crumbling knowledge or memory for tasks later in the industry. It's about booting up a brain. And I think physics uniquely boots up the brain the best. Because some of the things that they get you to do in your brain during physics is extremely valuable later. The idea of building models and abstractions and understanding that there are, there's a first order of approximation that describes most of the system. But then there's a second order, third order, first order terms that may or may not be present. And the idea that you're observing like a very noisy system, but actually there's like these fundamental frequencies that you can abstract away. Like when a physicist walks into the class and they say, assume there's a spherical cow and dot, dot, dot. And everyone laughs at that, but actually this is brilliant. It's brilliant thinking. That's very generalizable across the industry. Because yeah, cows can be approximated as a sphere, I guess, in a bunch of ways. There's a really good book, for example, Scale. It's basically from a physicist talking about biology. And maybe this is also a book I would recommend reading. But you can actually get a lot of really interesting approximations and chart scaling laws of animals. And you can look at their heartbeats and things like that. And they actually line up with the size of the animal and things like that. You can talk about an animal as a volume and you can actually derive a lot of, you can talk about the heat dissipation of that. Because your heat dissipation grows as the surface area, which is growing a square. But your heat creation or generation is growing as a cube. And so I just feel like physicists have all the right cognitive tools to approach problem solving in the world. So I think because of that training, I always try to find the first order terms or the second order terms of everything. When I'm observing a system or thing, I have a tangle of a web of ideas or knowledge in my world, in my mind. And I'm trying to find what is the thing that actually matters? What is the first order component? How can I simplify it? How can I have a simplest thing that actually shows that thing, right? That shows an action. And then I can tackle in the other terms. Maybe an example from one of my repos that I think illustrates it well is called MicroGrad. I don't know if you're familiar with this. So MicroGrad is 100 lines of code that shows back propagation. It can, you can create neural networks out of simple operations like plus and times, et cetera, like a blocks of neural networks. And you build up a computational graph and you do a forward pass and a backward pass to get the gradients. Now this is at the heart of all neural network learning. So MicroGrad is a 100 lines of pretty interpretable Python code. And it can do forward and backward arbitrary neural networks, but not efficiently. So MicroGrad, these 100 lines of Python are everything you need to understand how neural networks train. Everything else is just efficiency. Everything else is efficiency. And there's a huge amount of work to do efficiency. You know, you need your tensors, you laid them out, you stride them, you make sure your kernels are orchestrating memory movement correctly, et cetera. It's all just efficiency, roughly speaking. But the core intellectual sort of piece of neural network training is MicroGrad. It's 100 lines, you can easily understand it. Your chaining, it's a recursive application of chain rule to derive the gradient, which allows you to optimize any arbitrary differential function. So it's a, I love finding these like, you know, the smaller the terms and serving them on a platter and discovering them. And I feel like education is like the most intellectually interesting thing because you have a tangle of understanding and you're trying to lay it out in a way that creates a ramp where everything only depends on the thing before it. And I find that this like, you know, untangling of knowledge is just so intellectually interesting as a cognitive task. And so I love doing it personally, but I just find I have fascination with trying to lay things out in a certain way, maybe that helps me. It also just makes a learning experience so much more motivated. Your tutorial on the transformer begins with bigrams, literally like a lookup table from, here's the word right now, or here's the previous word, here's the next word, and it's literally just a lookup table. Yes, the essence of it, yeah. I mean, it's such a brilliant way, like, okay, start with a lookup table and then go to a transformer and then each piece is motivated. Why would you add that? Why would you add the next thing? You could memorize this sort of attention formula which is like having an understanding of why this is, every single piece is relevant, what problem it solves. Yeah, yeah. Yeah, you're presenting the pain before you present a solution and how clever is that? And you want to take the student through that progression. So there's a lot of other small things like that that I think make it nice and engaging and interesting. And always prompting the student. There's a lot of small things like that that I think are important and a lot of good educators will do, like how would you solve this? Like, I'm not gonna present a solution before you're gonna guess. That would be wasteful. That would be, that's a little bit of a, I don't want to swear, but it's a dick move towards you to present you with the solution before I give you a shot to try to come up with it yourself. Yeah, yeah. And then because if you try to come up with it yourself, I guess you get a better understanding of like, what is the action space? And then what is the sort of like objective? Then like, why does only this action fulfill that objective, right? Yeah, well, you have a chance to like try yourself and you have an appreciation when I give you the solution. And it maximizes the amount of knowledge per new fact added. That's right, yeah, yeah. Why do you think by default, people who are genuine experts in their field are often bad at explaining it to somebody ramping up? Well, it's the curse of knowledge and expertise. This is a real phenomenon and I actually suffered from it myself as much as I try to not suffer from it. But you take certain things for granted and you can't put yourself in the shoes of new, of people who are just starting out. And this is pervasive, it happens to me as well. One thing that I actually think is extremely helpful, as an example, someone was trying to show me a paper in biology recently. I just had instantly so many terrible questions. So what I did was I used ChachiPT to ask the questions with the paper in the context window. And then it worked through some of the simple things. And then I actually shared the thread to the person who shared it, who actually wrote that paper or worked on that work. And I almost feel like it was like, if they can see the dumb questions I had, it might help them explain it better in the future or something like that. Because, so for example, for my material, I would love if people shared their dumb conversations, with ChachiPT about the stuff that I've created, because it really helps me put myself again, in the shoes of someone who's starting out. Another trick like that, that I just works astoundingly well. If somebody writes a paper or a blog post or an announcement, it is in a hundred percent of cases true that just the narration or the transcription of how they would explain it to you over lunch is way more, not only understandable, but actually also more accurate and scientific, in the sense that people have a bias to explain things in the most abstract, jargon-filled way possible and to clear their throat for four paragraphs before they explain the central idea. But there's something about communicating one-on-one with a person, which compels you to just say the thing. Just say the thing. Actually, I saw that tweet. I thought it was really good. I shared it with a bunch of people, actually. I think it was really good. And I noticed this many, many times. Maybe the most prominent example is, I remember back in my PhD days doing research, et cetera, you read someone's paper, right? And you work to understand what it's doing, et cetera. And then you catch them, you're having beers at the conference later, and you ask them, so like this paper, like, so what were you doing? Like, what is the paper about? And they will just tell you these like three sentences that like perfectly captured the essence of that paper and totally give you the idea. And you didn't have to read the paper. Yeah, yeah, yeah. When you're sitting at the table with a beer or something like that, and like, oh yeah, the paper is just, oh, you take this idea, you take that idea, and you try this experiment, and you try this thing. And they have a way of just putting it conversationally and just like perfectly, like, why isn't that the abstract? Exactly. Yeah. Yeah. This is coming from the perspective of how somebody who's trying to explain an idea should formulate it better. What is your advice as a student to other students where if you don't have a carpathy who is doing the exposition of an idea, if you're reading a paper from somebody or reading a book, what strategies do you employ to learn material you're interested in, in fields you're not an expert in? I don't actually know that I have like unique tips and tricks, to be honest. Basically, it's kind of a painful process. But, you know, like redraft one. I think like one thing that has always helped me quite a bit is I had a small tweet about this actually. So like learning things on demand is pretty nice. Learning depth wise. I do feel like you need a bit of alternation of learning depth wise on demand. You're trying to achieve a certain project that you're gonna get a reward from. And learning breadth wise, which is just, oh, let's do whatever one-on-one. And here's all the things you might need, which is a lot of school. There's a lot of breadth wise learning. Like, oh, trust me, you'll need this later. You know, that kind of stuff. Like, okay, I trust you. I'll learn it because I guess I need it. But I love the kind of learning where you'll actually get a reward out of doing something and you're learning on demand. The other thing that I've found is extremely helpful is maybe this is an aspect where education is a bit more selfless. Because explaining things to people is a beautiful way to learn something more deeply. This happens to me all the time. I think it probably happens to other people too. Because I realize if I don't really understand something, I can't explain it, you know? And I'm trying and I'm like, actually, I don't understand this. And it's so annoying to come to terms with that. And then you can go back and make sure you understood it. And so it fills these gaps of your understanding. It forces you to come to terms with them and to reconcile them. I love to re-explain and things like that. And I think people should be doing that more as well. I think that forces you to manipulate the knowledge and make sure that you know what you're talking about when you're explaining it. Oh yeah, I think that's an excellent note to close on. Yeah. Andre, that was great. Yeah, thank you. Thanks. Have a good time. Hey everybody, I hope you enjoyed that episode. If you did, the most helpful thing you can do is just share it with other people who you think might enjoy it. It's also helpful if you leave a rating or a comment on whatever platform you're listening on. If you're interested in sponsoring the podcast, you can reach out at dwarkesh.com slash advertise. Otherwise, I'll see you on the next one.\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways to split a document. For this example, we'll use a simple splitter that splits the document into chunks of a fixed size. Check [Text Splitters](https://python.langchain.com/docs/how_to/#text-splitters) for more information about different approaches to splitting documents.\n",
    "\n",
    "For illustration purposes, let's split the transcription into chunks of 100 characters with an overlap of 20 characters and display the first few chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content='Reinfersional learning is terrible. It just so happens that everything that we had before is much'),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"had before is much worse. I'm actually optimistic. I think this will work. I think it's tractable.\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"it's tractable. I'm only sounding pessimistic because when I go on my Twitter timeline, I see all\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content='timeline, I see all this stuff. That makes no sense to me. A lot of it is, I think, honestly just'),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"honestly just fundraising. We're not actually building animals. We're building ghosts. These sort\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"ghosts. These sort of ethereal spirit entities, because they're fully digital, and they're kind of\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"and they're kind of like mimicking humans. And it's a different kind of intelligence. It's business\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"It's business as usual, because we're in an intelligence explosion already and have been for\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content='and have been for decades. Everything is gradually being automated, has been for hundreds of years.'),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"hundreds of years. Don't write blog posts. Don't do slides. Don't do any of that. Build the code.\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text_splitter.split_documents(text_documents)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our specific application, let's use 1000 characters instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the relevant chunks\n",
    "\n",
    "Given a particular question, we need to find the relevant chunks from the transcription to send to the model. Here is where the idea of **embeddings** comes into play.\n",
    "\n",
    "An embedding is a mathematical representation of the semantic meaning of a word, sentence, or document. It's a projection of a concept in a high-dimensional space. Embeddings have a simple characteristic: The projection of related concepts will be close to each other, while concepts with different meanings will lie far away. You can use the [Cohere's Embed Playground](https://dashboard.cohere.com/playground/embed) to visualize embeddings in two dimensions.\n",
    "\n",
    "To provide with the most relevant chunks, we can use the embeddings of the question and the chunks of the transcription to compute the similarity between them. We can then select the chunks with the highest similarity to the question and use them as the context for the model:\n",
    "\n",
    "<img src='images/system3.png' width=\"1200\">\n",
    "\n",
    "Let's generate embeddings for an arbitrary query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "[-0.010090172290802002, -0.0172368586063385, -0.00587761914357543, -0.01030843984335661, -0.006211255677044392, 0.017897896468639374, -0.010620249435305595, -0.02469535358250141, 0.001753931399434805, -0.02579292468726635]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(\"Where is this video being taken?\")\n",
    "\n",
    "print(f\"Embedding length: {len(embedded_query)}\")\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how embeddings work, let's first generate the embeddings for two different sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = embeddings.embed_query(\"Mary's sister is Susana\")\n",
    "sentence2 = embeddings.embed_query(\"This video shows the Andes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
    "\n",
    "We can use [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to calculate the similarity between the query and each of the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7238114027498846, 0.8280962306940765)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
    "\n",
    "query_sentence1_similarity, query_sentence2_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Vector Store\n",
    "\n",
    "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a **vector store**.\n",
    "\n",
    "A vector store is a database of embeddings that specializes in fast similarity searches. \n",
    "\n",
    "<img src='images/system4.png' width=\"1200\">\n",
    "\n",
    "To understand how a vector store works, let's create one in memory and add a few embeddings to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"Mary's sister is Susana\",\n",
    "        \"John and Tommy are brothers\",\n",
    "        \"Patricia likes white cars\",\n",
    "        \"Pedro's mother is a teacher\",\n",
    "        \"Lucia drives an Audi\",\n",
    "        \"Mary has two siblings\",\n",
    "    ],\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query the vector store to find the most similar embeddings to a given query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"Mary's sister is Susana\"), 0.917323542883913),\n",
       " (Document(page_content='Mary has two siblings'), 0.9045029978848259),\n",
       " (Document(page_content='John and Tommy are brothers'), 0.8013182122337675)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore1.similarity_search_with_score(query=\"Who is Mary's sister?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the vector store to the chain\n",
    "\n",
    "We can use the vector store to find the most relevant chunks from the transcription to send to the model. Here is how we can connect the vector store to the chain:\n",
    "\n",
    "<img src='images/chain4.png' width=\"1200\">\n",
    "\n",
    "We need to configure a [Retriever](https://python.langchain.com/docs/how_to/#retrievers). The retriever will run a similarity search in the vector store and return the most similar documents back to the next step in the chain.\n",
    "\n",
    "We can get a retriever directly from the vector store we created before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Mary's sister is Susana\"),\n",
       " Document(page_content='Mary has two siblings'),\n",
       " Document(page_content='John and Tommy are brothers'),\n",
       " Document(page_content=\"Pedro's mother is a teacher\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever1 = vectorstore1.as_retriever()\n",
    "retriever1.invoke(\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prompt expects two parameters, \"context\" and \"question.\" We can use the retriever to find the chunks we'll use as the context to answer the question.\n",
    "\n",
    "We can create a map with the two inputs by using the [`RunnableParallel`](https://python.langchain.com/docs/how_to/parallel/) and [`RunnablePassthrough`](https://python.langchain.com/docs/how_to/passthrough/) classes. This will allow us to pass the context and question to the prompt as a map with the keys \"context\" and \"question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='Patricia likes white cars'),\n",
       "  Document(page_content='Lucia drives an Audi'),\n",
       "  Document(page_content=\"Pedro's mother is a teacher\"),\n",
       "  Document(page_content=\"Mary's sister is Susana\")],\n",
       " 'question': \"What color is Patricia's car?\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
    "setup.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add the setup map to the chain and run it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup | prompt | model | parser\n",
    "chain.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the chain using another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lucia drives an Audi.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What car does Lucia drive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading transcription into the vector store\n",
    "\n",
    "We initialized the vector store with a few random strings. Let's create a new vector store using the chunks from the video transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore2 = DocArrayInMemorySearch.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'transcription.txt'}, page_content=\"everything at once. Animals are maybe a better example, because they don't even have the scaffold of language. They just get thrown out into the world. And they just have to make sense of everything without any labels. And the vision for AGI then should just be something which just looks at sensory data, looks at the computer screen, and it just figures out what's going on from scratch. I mean, if a human was put in a similar situation, that would be trained from scratch. But I mean, this is like a human growing up, or an animal growing up. So why shouldn't that be the vision for AI rather than this thing where we're doing millions of years of training? I think that's a really good question. I mean, so Sutton was on your podcast, and I saw the podcast. And I had a write up about that podcast almost that gets into a little bit of how I see things. And I kind of feel like I'm very careful to make analogies to animals, because they came about by very different optimization process.\")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a new chain using the correct vector store. This time we are using a different equivalent syntax to specify the [`RunnableParallel`](https://python.langchain.com/docs/how_to/parallel/) portion of the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMs are different because they do not have the equivalent of culture, they are extremely good at memorization, and they lack the ability to learn abstract concepts quickly like a child can.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": vectorstore2.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"Why are LLMs Different?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Pinecone\n",
    "\n",
    "So far we've used an in-memory vector store. In practice, we need a vector store that can handle large amounts of data and perform similarity searches at scale. For this example, we'll use [Pinecone](https://www.pinecone.io/).\n",
    "\n",
    "The first step is to create a Pinecone account, set up an index, get an API key, and set it as an environment variable `PINECONE_API_KEY`.\n",
    "\n",
    "Then, we can load the transcription documents into Pinecone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cesaranasco/Documents/EMPRESAS/DEVDUO/youtube-rag/.venv2/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = \"youtube-rag-llm\"\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents, embeddings, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run a similarity search on a pinecone to make sure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content=\"geniuses of today are bare discussion to surface of what a human mind can do, I think. Today, I'm speaking with Andre Karpathy. Andre, why do you say that this will be the decade of agents and not the year of agents? Well, first of all, thank you for having me here. Excited to be here. So the quote that you've just mentioned, it's the decade of agents, that's actually a reaction to an existing pre-existing quote, I should say, where I think some of the labs, I'm not actually sure who said this, but they were alluding to this being the year of agents with respect to LLMs and how they were going to evolve. And I think I was triggered by that, because I feel like there's some overpredictions going on in the industry. And in my mind, this is really a lot more accurately described as the decade of agents. And we have some very early agents that are actually extremely impressive that I use daily, Claude and Codex and so on. But I still feel like there's so much work to be done. And so I\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"subject on the side. That's kind of maybe like the first dramatic sort of seismic shift that came with the AlexNet and so on. I would say AlexNet reoriented everyone, and everyone started to train neural networks. But it was still very per task, per specific task. So maybe I have an image classifier, or I have a neural machine translator, or something like that. And people became very slowly actually interested in basically agents, I would say. And people started to think, OK, well, maybe we have a check mark next to the visual cortex, or something like that. But what about the other parts of the brain? And how can we get an actual full agent or full entity that can actually interact in the world? And I would say the Atari sort of deep reinforcement learning shift in 2013 or so was part of that early effort of agents in my mind. Because it was an attempt to try to get agents that not just perceive the world, but also take actions and interact and get rewards from environments. And at\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"go through each of them one by one? Yeah, I mean, that's a giant question, because of course, you're talking about 15 years of stuff that happened. I mean, AI is actually so wonderful, because there have been a number of, I would say, seismic shifts that where the entire field has suddenly looked a different way. And I guess I've maybe lived through two or three of those. And I still think there will continue to be some, because they come with some kind of almost surprising regularity. Well, when my career began, of course, when I started to work on deep learning, when I became interested in deep learning, this was just kind of like by chance of being right next to Jeff Hinton at the University of Toronto. And Jeff Hinton, of course, is kind of like the godfather figure of AI. And he was training all these neural networks, and I thought it was incredible and interesting. But this was not like the main thing that everyone in AI was doing by far. This was a niche little subject on the\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.similarity_search(\"Who is Andrej Karpathy?\")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup the new chain using Pinecone as the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The people in the podcast are discussing the advancements and challenges in artificial intelligence, particularly focusing on the development of agents and the need for continual learning in AI systems. They also touch upon the importance of clear and direct communication in explaining complex ideas.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What do this people in the podcast talking about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Andre Karpathy and the interviewer are the people talking in the podcast.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who are the poeple talking in the podcast?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Andre Karpathy's expertise is in the field of AI, specifically in training neural networks for tasks, developing agents, and working with LLMs (Large Language Models).\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is Andre Karpathy expertise?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
